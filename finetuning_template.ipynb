{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6ed459",
   "metadata": {},
   "source": [
    "# Fine-tuning Template Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a52c6-1e0e-4215-a424-3faecde70d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y torch torchaudio torchvision triton torchtext torchaudio bitsandbytes\n",
    "# !pip install -r finetuning_requirements.txt.txt --force-reinstall --no-cache-dir\n",
    "# !pip install torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -U boto3==1.34.69 botocore==1.34.69 aiobotocore==2.12.3 s3fs==2024.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a644d0-a2eb-4093-a333-85bb6cad2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import triton\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"bitsandbytes:\", bnb.__version__)\n",
    "print(\"Triton:\", triton.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b428ab-8655-419c-a3fd-84e732f6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727a8bf-8355-4c2f-9a12-8ebcb31383dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datasets import Audio, Dataset, IterableDataset, load_dataset, load_from_disk\n",
    "from jiwer import wer\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acefb9-9f18-4756-926a-42183e55ccc7",
   "metadata": {},
   "source": [
    "### Project Configuration\n",
    "- Be sure to set the `PROJECT_NAME` var with your `name/experiment_description` so we can keep track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a22a57-4099-46a9-a26f-2348e663bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Config\n",
    "PROJECT_NAME = \"dave/EXPERIMENT_NAME_HERE\"\n",
    "BUCKET = \"asrelder-data\"\n",
    "CLIPS_PREFIX = \"common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\"\n",
    "OUTPUT_PREFIX = f\"experiments/{PROJECT_NAME}\"\n",
    "BASE_S3_PREFIX = f\"s3://{BUCKET}/{CLIPS_PREFIX}\"\n",
    "\n",
    "# Keys\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"FILL_ME_IN\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"FILL_ME_IN\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"FILL_ME_IN\"\n",
    "\n",
    "# Confirm\n",
    "!aws sts get-caller-identity\n",
    "\n",
    "# Test\n",
    "sts = boto3.client(\"sts\")\n",
    "print(sts.get_caller_identity())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6d900-7421-412d-b342-5030cbb87bd4",
   "metadata": {},
   "source": [
    "### Download the train, validation, and test CSVs from GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93d47e-c9d8-4f27-ab04-235f18038ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get csvs from GDrive\n",
    "DRIVE_FILE_IDS = {\n",
    "    \"train\": \"1AdCeMxDcE4rxqWSyPsfEh7TaS5dWjXD5\",  # common_voices_23_balanced_on_60.csv\n",
    "    \"val\": \"1GzrujHvGwA7MA9awtQI4IFRQIdYcLiBO\",  # common_voices_23_val_full.csv\n",
    "    \"test\": \"1bSjhB8WTDZWBTuppB-vU56AgOEzNNAeN\",  # common_voices_23_test_full.csv\n",
    "}\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "def download_from_drive(name, file_id):\n",
    "    out_path = f\"data/{name}.csv\"\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    print(f\"Downloading {name} split from Google Drive ‚Üí {out_path}\")\n",
    "    subprocess.run([\"gdown\", \"--fuzzy\", url, \"-O\", out_path], check=True)\n",
    "    return out_path\n",
    "\n",
    "TRAIN_PATH = download_from_drive(\"train\", DRIVE_FILE_IDS[\"train\"])\n",
    "VAL_PATH = download_from_drive(\"val\", DRIVE_FILE_IDS[\"val\"])\n",
    "TEST_PATH = download_from_drive(\"test\", DRIVE_FILE_IDS[\"test\"])\n",
    "\n",
    "DATA_FILES = {\n",
    "    \"train\": TRAIN_PATH,\n",
    "    \"val\": VAL_PATH,\n",
    "    \"test\": TEST_PATH,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eb6883-fc3d-4a8d-91a4-8250eb306dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the columns for train, val, and test\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    path = f\"data/{split}.csv\"\n",
    "    df = pd.read_csv(path, nrows=1)\n",
    "    print(f\"{path} columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabb79d-277e-4ae7-8162-c1df48843086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add age_group for val and test; column present in train\n",
    "def add_age_group_column(file_path: str):\n",
    "    \"\"\"\n",
    "    Adds 'age_group' column to the CSV file if missing\n",
    "    Derives it from the 'age' column (e.g., '23' -> '20')\n",
    "    NOTE: Modifies the file in place\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    if \"age_group\" in df.columns:\n",
    "        print(f\"Skipping, 'age_group' already exists in {file_path}\")\n",
    "        return\n",
    "\n",
    "    def infer_age_group(age_value):\n",
    "        if pd.isna(age_value):\n",
    "            return \"\"\n",
    "        # Normalize to string\n",
    "        s = str(age_value).strip().lower()\n",
    "        # Handle numeric (e.g. 23)\n",
    "        if re.match(r\"^\\d{2}$\", s):\n",
    "            decade = int(s) // 10 * 10\n",
    "            return f\"{decade}s\"\n",
    "        # Handle ranges like \"25-34\"\n",
    "        match = re.match(r\"(\\d{2})\\s*-\\s*(\\d{2})\", s)\n",
    "        if match:\n",
    "            decade = int(match.group(1)) // 10 * 10\n",
    "            return f\"{decade}s\"\n",
    "        # Handle words like 'twenties', 'forty', etc.\n",
    "        words_to_decade = {\n",
    "            \"teen\": \"10s\", \"teens\": \"10s\",\n",
    "            \"twenty\": \"20s\", \"twenties\": \"20s\",\n",
    "            \"thirty\": \"30s\", \"thirties\": \"30s\",\n",
    "            \"forty\": \"40s\", \"forties\": \"40s\",\n",
    "            \"fifty\": \"50s\", \"fifties\": \"50s\",\n",
    "            \"sixty\": \"60s\", \"sixties\": \"60s\",\n",
    "            \"seventy\": \"70s\", \"seventies\": \"70s\",\n",
    "            \"eighty\": \"80s\", \"eighties\": \"80s\",\n",
    "        }\n",
    "        for k, v in words_to_decade.items():\n",
    "            if k in s:\n",
    "                return v\n",
    "        return \"\"  # unknown or other format\n",
    "\n",
    "    df[\"age_group\"] = df[\"age\"].apply(infer_age_group)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Added 'age_group' to {file_path} ({len(df)} rows)\")\n",
    "\n",
    "add_age_group_column(\"data/val.csv\")\n",
    "add_age_group_column(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f12b8b-95ea-47cf-b59b-c3a6ca973dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_s3_paths(file_path: str):\n",
    "    \"\"\"\n",
    "    Prepends full S3 URI prefix to the 'path' column\n",
    "    Modifies the file in place\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Processing {file_path} ({len(df)} rows)\")\n",
    "    if \"path\" in df.columns:\n",
    "        def prepend_prefix(p):\n",
    "            if isinstance(p, str) and not p.startswith(\"s3://\"):\n",
    "                return f\"{BASE_S3_PREFIX}/{p.lstrip('/')}\"\n",
    "            return p\n",
    "        df[\"path\"] = df[\"path\"].apply(prepend_prefix)\n",
    "        print(\"Updated 'path' column with S3 prefix\")\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved updated CSV: {file_path}\\n\")\n",
    "    print(f\"Updated 'path' for {file_path} ({len(df)} rows)\")\n",
    "\n",
    "# Apply to val/test (train likely already correct)\n",
    "add_s3_paths(\"data/train.csv\")\n",
    "add_s3_paths(\"data/val.csv\")\n",
    "add_s3_paths(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669aaa8d-d259-41b6-99a3-c2ec6c23ff88",
   "metadata": {},
   "source": [
    "### Load Whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d566d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper model + language/task settings\n",
    "WHISPER_MODEL = \"openai/whisper-base\"\n",
    "LANGUAGE = \"en\"\n",
    "TASK = \"transcribe\"\n",
    "\n",
    "# Compute and training\n",
    "MIXED_PRECISION = \"fp16\"\n",
    "GRADIENT_ACCUMULATION = 2\n",
    "BATCH_SIZE_PER_DEVICE = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.0001\n",
    "MAX_AUDIO_SECONDS = 30\n",
    "\n",
    "# PEFT method\n",
    "# NOTE: qlora, dora, adalora, none\n",
    "PEFT_METHOD = \"qlora\"\n",
    "\n",
    "# ReFT toggle (stubbed ‚Äì see the ReFT cell for instructions)\n",
    "ENABLE_REFT = False\n",
    "\n",
    "print({\n",
    "    \"python\": platform.python_version(),\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"device_count\": torch.cuda.device_count(),\n",
    "    \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    \"time\": dt.datetime.now().isoformat(timespec=\"seconds\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86148503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Utils\n",
    "# Detect SageMaker environment, map S3 paths\n",
    "\n",
    "def in_sagemaker() -> bool:\n",
    "    return any(k.startswith(\"SM_\") for k in os.environ.keys()) or os.environ.get(\"SAGEMAKER_JOB_NAME\") is not None\n",
    "\n",
    "def s3_join(*parts: str) -> str:\n",
    "    return \"/\".join([p.strip(\"/\").replace(\"s3://\",\"\") for p in parts])\n",
    "\n",
    "print(\"In SageMaker:\", in_sagemaker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c47ce1-bdaa-4a8b-a95a-49284fbfb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "def clean_numeric_columns(file_path: str, numeric_cols=(\"variant\", \"segment\")):\n",
    "    df = pd.read_csv(file_path)\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Cleaned {file_path}: non-numeric entries coerced to NaN.\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    clean_numeric_columns(f\"data/{split}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352726d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Loading\n",
    "# NOTE: Went with a streaming approach (streaming=True) rather than copying over 150,000 audio clips\n",
    "# raw_datasets = load_dataset(\"csv\", data_files=DATA_FILES)\n",
    "\n",
    "# Streaming Load from S3\n",
    "print(\"Loading CSVs in streaming mode...\")\n",
    "audio_col = \"path\"\n",
    "text_col = \"sentence\"\n",
    "raw_datasets = load_dataset(\"csv\", data_files=DATA_FILES, streaming=True)\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d2124-3f28-48e3-a187-ebec8bdf6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use half of available CPUs\n",
    "print(f\"CPU count: {os.cpu_count()}\")\n",
    "NUM_PROC = max(1, os.cpu_count() // 2)\n",
    "\n",
    "# Cache the audio files so that we don't have to stream them from S3\n",
    "CACHE_DIR = \"data/processed_whisper\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfaa115-9563-4ce7-a390-e2195bb246f9",
   "metadata": {},
   "source": [
    "### Whisper Processor & Preprocessing\n",
    "\n",
    "Prepare tensors for each audio clip to send to model's encoder\n",
    "- Load audio from S3 or disk\n",
    "- Resample to 16 kHz\n",
    "- Compute short-time Fourier transform (STFT)\n",
    "- Convert to Mel scale\n",
    "- Apply log compression\n",
    "- Normalize to match Whisper's training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c31a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper Processor & Preprocessing\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    WHISPER_MODEL,\n",
    "    language=LANGUAGE,\n",
    "    task=TASK\n",
    ")\n",
    "feature_extractor: WhisperFeatureExtractor = processor.feature_extractor\n",
    "tokenizer: WhisperTokenizer = processor.tokenizer\n",
    "MAX_INPUT_LENGTH = int(MAX_AUDIO_SECONDS * feature_extractor.sampling_rate)\n",
    "\n",
    "# Narrow to only the audio path and text columns\n",
    "# NOTE: Peek at one example to infer columns\n",
    "first_example = next(iter(raw_datasets[\"train\"]))\n",
    "column_names = list(first_example.keys())\n",
    "print(\"Detected columns:\", column_names)\n",
    "remove_columns = [c for c in column_names if c not in (audio_col, text_col)]\n",
    "print(\"Removed columns:\", remove_columns)\n",
    "\n",
    "# Preprocessing Function\n",
    "def prepare_example(batch):\n",
    "    audio = batch[audio_col]\n",
    "    # HF Audio feature will lazily decode from S3 on the fly\n",
    "    if isinstance(audio, dict) and \"array\" in audio:\n",
    "        arr = audio[\"array\"]\n",
    "        sr = audio.get(\"sampling_rate\", 16000)\n",
    "    else:\n",
    "        arr = audio[\"array\"]\n",
    "        sr = audio[\"sampling_rate\"]\n",
    "\n",
    "    if arr.shape[0] > MAX_INPUT_LENGTH:\n",
    "        arr = arr[:MAX_INPUT_LENGTH]\n",
    "\n",
    "    inputs = feature_extractor(arr, sampling_rate=sr)\n",
    "    labels = tokenizer(batch[text_col]).input_ids\n",
    "    return {\n",
    "        \"input_features\": inputs[\"input_features\"][0],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Testing Mode (subset for quick iteration)\n",
    "TESTING_MODE = True\n",
    "\n",
    "if TESTING_MODE:\n",
    "    print(\"Running in TESTING_MODE (100 samples)...\")\n",
    "    small_train = raw_datasets[\"train\"].take(100)  # streaming-safe slice\n",
    "    small_train = Dataset.from_generator(lambda: small_train)\n",
    "    small_train = small_train.cast_column(audio_col, Audio(sampling_rate=16000))\n",
    "\n",
    "    processed = small_train.map(\n",
    "        prepare_example,\n",
    "        remove_columns=remove_columns,\n",
    "        desc=\"Processing Whisper sample\",\n",
    "    )\n",
    "    print(processed)\n",
    "    processed.save_to_disk(CACHE_DIR)\n",
    "    print(f\"üíæ Cached small sample to {CACHE_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüöÄ Running full preprocessing stream (this may take a while)...\\n\")\n",
    "    start = time.time()\n",
    "    streamed = raw_datasets[\"train\"].cast_column(audio_col, Audio(sampling_rate=16000))\n",
    "    processed = streamed.map(\n",
    "        prepare_example,\n",
    "        remove_columns=remove_columns,\n",
    "        desc=\"Processing Whisper full stream\"\n",
    "    )\n",
    "    print(f\"‚è± Completed in {(time.time() - start)/60:.2f} min\")\n",
    "    processed.save_to_disk(CACHE_DIR)\n",
    "    print(f\"Cached full stream to {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56e9e7-e1b0-4f14-a4b4-c7b339d59d76",
   "metadata": {},
   "source": [
    "### Load the processed tensors from disk (inside data/ directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135933b-9fe9-447e-9c42-57ab45389e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed tensors from disk (inside data/ directory)\n",
    "proc_datasets = load_from_disk(\"data/processed_whisper\")\n",
    "print(proc_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217cad9e-5f76-4984-af2b-696b6f71396b",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338cf31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & PEFT Setup\n",
    "# NOTE: Do not use `task_type`, it causes 'input_ids' issues\n",
    "\n",
    "bnb_config = None\n",
    "load_in_4bit = False\n",
    "\n",
    "if PEFT_METHOD == \"qlora\":\n",
    "    load_in_4bit = True\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "quant_kwargs = dict(device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "if bnb_config is not None:\n",
    "    quant_kwargs[\"quantization_config\"] = bnb_config\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    WHISPER_MODEL,\n",
    "    **quant_kwargs,\n",
    ")\n",
    "\n",
    "# Gradient checkpointing is very helpful for Whisper fine-tuning\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare model for k-bit training if using QLoRA\n",
    "if load_in_4bit:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define which modules to apply LoRA on for Whisper\n",
    "# NOTE: I don't know that these are the \"right\" layers, they're from: https://github.com/openai/whisper/discussions/830\n",
    "whisper_layers = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"out_proj\",\n",
    "    \"fc1\",\n",
    "    \"fc2\",\n",
    "]\n",
    "\n",
    "# Choose PEFT config\n",
    "peft_config = None\n",
    "if PEFT_METHOD == \"qlora\":\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        # task_type=\"SEQ_2_SEQ_LM\",\n",
    "        target_modules=whisper_layers,\n",
    "    )\n",
    "elif PEFT_METHOD == \"dora\":\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        # task_type=\"SEQ_2_SEQ_LM\",\n",
    "        target_modules=whisper_layers,\n",
    "        use_dora=True,  # DoRA flag in PEFT\n",
    "    )\n",
    "elif PEFT_METHOD == \"adalora\":\n",
    "    peft_config = AdaLoraConfig(\n",
    "        init_r=12,   # starting rank\n",
    "        target_r=8,  # target rank after adaptation\n",
    "        beta1=0.85,\n",
    "        beta2=0.85,\n",
    "        tinit=200,\n",
    "        tfinal=1000,\n",
    "        deltaT=10,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        orth_reg_weight=0.5,\n",
    "        target_modules=whisper_layers,\n",
    "        # task_type=\"SEQ_2_SEQ_LM\",\n",
    "    )\n",
    "else:\n",
    "    print(\"PEFT_METHOD == 'none' ‚Üí full‚Äëparameter fine‚Äëtuning (not recommended on small GPUs).\")\n",
    "    raise Exception(\"We are not supporting full parameter fine tuning right now. Provide a supported PEFT_METHOD.\")\n",
    "\n",
    "if peft_config is not None:\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6785689-1fe4-47d8-b468-c472f7194ecf",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Training\n",
    "\n",
    "# Metric\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return {\"wer\": wer(label_str, pred_str)}\n",
    "\n",
    "# Force language/task tokens for Whisper decoding\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=LANGUAGE, task=TASK)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./outputs/{PROJECT_NAME}\",\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE_PER_DEVICE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    fp16=(MIXED_PRECISION == \"fp16\"),\n",
    "    bf16=(MIXED_PRECISION == \"bf16\"),\n",
    "    report_to=[\"none\"],  # or ['tensorboard']\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "def data_collator(features):\n",
    "    input_features = torch.stack([torch.tensor(f[\"input_features\"]) for f in features])\n",
    "    label_batch = [f[\"labels\"] for f in features]\n",
    "    labels = tokenizer.pad(\n",
    "        {\"input_ids\": label_batch},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n",
    "# Detect whether it's a DatasetDict or a single Dataset\n",
    "if isinstance(proc_datasets, dict) or hasattr(proc_datasets, \"keys\"):\n",
    "    train_data = proc_datasets.get(\"train\") or proc_datasets[\"train\"]\n",
    "    eval_data = proc_datasets.get(\"validation\") or proc_datasets.get(\"val\") or None\n",
    "else:\n",
    "    # Single dataset ‚Äî just use it for both train/eval if in testing mode\n",
    "    train_data = proc_datasets\n",
    "    eval_data = proc_datasets\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "# trainer = WhisperSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    tokenizer=processor.feature_extractor,  # logs shapes properly\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0916f-0bdb-4818-bef2-6ee1d2e2ad20",
   "metadata": {},
   "source": [
    "### Evaluation and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Split\n",
    "test_metrics = {}\n",
    "if \"test\" in proc_datasets:\n",
    "    test_metrics = trainer.evaluate(proc_datasets[\"test\"], metric_key_prefix=\"test\")\n",
    "    print(test_metrics)\n",
    "else:\n",
    "    print(\"No test split available in proc_datasets. Skipping.\")\n",
    "\n",
    "# Save metrics\n",
    "os.makedirs(\"metrics\", exist_ok=True)\n",
    "now = dt.datetime.utcnow().strftime(\"%Y-%m-%d%H:%M:%S\")\n",
    "with open(f\"metrics/results_{now}.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\"eval\": trainer.state.log_history, \"test\": test_metrics},\n",
    "        f,\n",
    "        indent=2\n",
    "    )\n",
    "print(\"Saved metrics to metrics/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload Artifacts to S3\n",
    "\n",
    "# def aws_cp(local_path: str, s3_uri: str):\n",
    "#     cmd = [\"aws\",\"s3\",\"cp\",\"--recursive\", local_path, s3_uri]\n",
    "#     print(\" \".join(cmd))\n",
    "#     try:\n",
    "#         subprocess.check_call(cmd)\n",
    "#     except Exception as e:\n",
    "#         print(\"Upload failed:\", e)\n",
    "\n",
    "# S3_OUTPUT_URI = f\"{S3_BUCKET}/{S3_OUTPUT_PREFIX}\".rstrip(\"/\")\n",
    "# aws_cp(\"./outputs\", f\"s3://{S3_OUTPUT_URI}/outputs\")\n",
    "# aws_cp(\"./metrics\", f\"s3://{S3_OUTPUT_URI}/metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcd7ee-4b80-4706-893d-d36709e59da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
