{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TIW9Veqa4Qp"
   },
   "outputs": [],
   "source": [
    "!pip install imageio-ffmpeg torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OF262IKa5o8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tempfile\n",
    "import shutil\n",
    "import subprocess\n",
    "import traceback\n",
    "import gc\n",
    "from typing import List, Set, Dict, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "from dataclasses import asdict\n",
    "from sagemaker.pytorch import PyTorchProcessor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingOutput\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0D1ADtOa5mI"
   },
   "outputs": [],
   "source": [
    "mp.set_start_method(\"spawn\", force=True) \n",
    "\n",
    "_PIPELINE = None\n",
    "_PIPELINE_DEVICE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nabJ9h9ua5jY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import torch first to avoid registration conflicts\n",
    "import torch\n",
    "import torchvision  # Import this explicitly before transformers\n",
    "\n",
    "# Env setup\n",
    "import os\n",
    "import imageio_ffmpeg, os\n",
    "os.environ[\"PATH\"] = os.path.dirname(imageio_ffmpeg.get_ffmpeg_exe()) + os.pathsep + os.environ.get(\"PATH\",\"\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/tmp/transformers_cache\")\n",
    "os.environ.setdefault(\"HF_HOME\", \"/tmp/hf_home\")\n",
    "os.environ.setdefault(\"TORCH_HOME\", \"/tmp/torch_home\")\n",
    "\n",
    "# Make sure feature extraction happens on CPU threads while GPU is decoding\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import imageio_ffmpeg\n",
    "from transformers import pipeline\n",
    "import argparse, os, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set the access and secret keys here so we don't have to deal with gnarly IAM stuff\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"FILL_ME_IN\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"FILL_ME_IN\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size and num workers here\n",
    "ASR_WORKERS = 1\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that having set S3 keys, we can indeed access the bucket\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(s3.list_objects_v2(Bucket=\"asrelder-data\")[\"KeyCount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmPJjB-ka5g4"
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(\"asr_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pdbTk39a5eG"
   },
   "outputs": [],
   "source": [
    "# =============================================== Config ===============================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the transcription pipeline\"\"\"\n",
    "    # S3 settings\n",
    "    s3_input: str = \"s3://asrelder-data/common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\"\n",
    "    output_local_csv: str = \"./transcripts_from_prefix.csv\"\n",
    "    write_back_to_s3: bool = False\n",
    "    output_s3_uri: Optional[str] = None\n",
    "    #output_s3_uri: str = \"s3://asrelder-data/outputs/transcripts_from_prefix.csv\" uncomment if we want it back on s3\n",
    "    validation_csv_path: str = \"common_voices_23_train_with_validated_votes.csv\"\n",
    "    validation_csv_column: str = \"path\"\n",
    "\n",
    "    # Processing settings\n",
    "    max_files: Optional[int] = 1500\n",
    "    download_workers: int = 8\n",
    "    append_every_n: int = 200\n",
    "    resume_from_csv: bool = True\n",
    "\n",
    "    # Model settings\n",
    "    model_id: str = \"openai/whisper-base\"\n",
    "    language: Optional[str] = \"en\"\n",
    "    task: str = \"transcribe\"\n",
    "    chunk_length_s: int = 30\n",
    "    stride_length_s: tuple = (5, 5)\n",
    "\n",
    "    # File settings\n",
    "    audio_extensions: List[str] = field(default_factory=lambda: [\".mp3\", \".wav\", \".flac\", \".m4a\", \".ogg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKouNU6ka5bj"
   },
   "outputs": [],
   "source": [
    "def _build_worker_pipeline(cfg_dict, device_id):\n",
    "    use_cuda = torch.cuda.is_available() and device_id >= 0\n",
    "    dtype = torch.float16 if use_cuda else torch.float32\n",
    "    generate_kwargs = {}\n",
    "    if cfg_dict.get(\"language\"): generate_kwargs[\"language\"] = cfg_dict[\"language\"]\n",
    "    if cfg_dict.get(\"task\"):     generate_kwargs[\"task\"] = cfg_dict[\"task\"]\n",
    "    return pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=cfg_dict[\"model_id\"],\n",
    "        device=(device_id if use_cuda else -1),\n",
    "        torch_dtype=dtype,\n",
    "        return_timestamps=True,\n",
    "        chunk_length_s=cfg_dict[\"chunk_length_s\"],\n",
    "        stride_length_s=tuple(cfg_dict[\"stride_length_s\"]),\n",
    "        generate_kwargs=(generate_kwargs or None),\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "def _transcribe_worker(args):\n",
    "    # args: (audio_path, cfg_dict, device_id)\n",
    "    audio_path, cfg_dict, device_id = args\n",
    "    global _PIPELINE, _PIPELINE_DEVICE\n",
    "    if (_PIPELINE is None) or (_PIPELINE_DEVICE != device_id):\n",
    "        _PIPELINE = _build_worker_pipeline(cfg_dict, device_id)\n",
    "        _PIPELINE_DEVICE = device_id\n",
    "    try:\n",
    "        out = _PIPELINE(audio_path)\n",
    "        text = out.get(\"text\", \"\") if isinstance(out, dict) else str(out)\n",
    "        return {\"path\": audio_path, \"text\": text, \"error\": \"\"}\n",
    "    except Exception as e:\n",
    "        return {\"path\": audio_path, \"text\": \"\", \"error\": f\"{type(e).__name__}: {e}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5J475Bqa5ZF"
   },
   "outputs": [],
   "source": [
    "# =============================================== Core Components ===============================================\n",
    "class FFmpegSetup:\n",
    "    \"\"\"Manages FFmpeg availability\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_available() -> Optional[str]:\n",
    "        \"\"\"Check if ffmpeg is available on PATH\"\"\"\n",
    "        ff = None\n",
    "        try:\n",
    "            ff = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"imageio-ffmpeg error: {e}\")\n",
    "\n",
    "        if ff and os.path.exists(ff):\n",
    "            ff_dir = os.path.dirname(ff)\n",
    "            os.environ[\"PATH\"] = ff_dir + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "        resolved = shutil.which(\"ffmpeg\")\n",
    "        if resolved:\n",
    "            try:\n",
    "                out = subprocess.run(\n",
    "                    [resolved, \"-version\"],\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.STDOUT,\n",
    "                    check=True,\n",
    "                    timeout=5\n",
    "                )\n",
    "                logger.info(f\"ffmpeg: {resolved} | {out.stdout.decode('utf-8').splitlines()[0]}\")\n",
    "            except Exception:\n",
    "                logger.info(f\"ffmpeg: {resolved} (version check failed)\")\n",
    "        else:\n",
    "            # Create shim if needed\n",
    "            if ff and os.path.exists(ff):\n",
    "                bin_dir = os.path.expanduser(\"~/.local/bin\")\n",
    "                os.makedirs(bin_dir, exist_ok=True)\n",
    "                shim = os.path.join(bin_dir, \"ffmpeg\")\n",
    "                with open(shim, \"w\") as f:\n",
    "                    f.write(f\"#!/usr/bin/env bash\\n\\\"{ff}\\\" \\\"$@\\\"\\n\")\n",
    "                os.chmod(shim, 0o755)\n",
    "                os.environ[\"PATH\"] = bin_dir + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "                resolved = shutil.which(\"ffmpeg\")\n",
    "                if resolved:\n",
    "                    logger.info(f\"ffmpeg shim created: {resolved}\")\n",
    "\n",
    "        if not resolved:\n",
    "            logger.warning(\"FFmpeg not found; use torchaudio fallback\")\n",
    "\n",
    "        return resolved\n",
    "\n",
    "class S3Manager:\n",
    "    \"\"\"Handles S3 operations\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.client = boto3.client(\"s3\")\n",
    "\n",
    "    def parse_uri(self, uri: str) -> tuple[str, str]:\n",
    "        \"\"\"Parse S3 URI into bucket and key\"\"\"\n",
    "        if not uri.startswith(\"s3://\"):\n",
    "            raise ValueError(f\"Invalid S3 URI: {uri}\")\n",
    "        p = urlparse(uri)\n",
    "        return p.netloc, p.path.lstrip(\"/\")\n",
    "\n",
    "    def is_audio_file(self, key: str) -> bool:\n",
    "        \"\"\"Check if key is an audio file\"\"\"\n",
    "        return any(key.lower().endswith(ext) for ext in self.config.audio_extensions)\n",
    "\n",
    "    def list_audio_keys(self, bucket: str, prefix: str, allowed_filenames: Optional[Set[str]] = None) -> List[str]:\n",
    "        \"\"\"List all audio keys under prefix, optionally filtered by allowed filenames\"\"\"\n",
    "        if self.is_audio_file(prefix):\n",
    "            return [prefix]\n",
    "\n",
    "        keys = []\n",
    "        paginator = self.client.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if not key.endswith(\"/\") and self.is_audio_file(key):\n",
    "                    if allowed_filenames:\n",
    "                        filename = os.path.basename(key)\n",
    "                        if filename not in allowed_filenames:\n",
    "                            continue\n",
    "                    keys.append(key)\n",
    "                    if self.config.max_files and len(keys) >= self.config.max_files:\n",
    "                        return keys\n",
    "        return keys\n",
    "\n",
    "    def download_to_temp(self, bucket: str, key: str) -> str:\n",
    "        \"\"\"Download S3 object to temporary file\"\"\"\n",
    "        _, ext = os.path.splitext(key)\n",
    "        if not ext:\n",
    "            ext = \".mp3\"\n",
    "\n",
    "        fd, tmp_path = tempfile.mkstemp(suffix=ext)\n",
    "        os.close(fd)\n",
    "\n",
    "        with open(tmp_path, \"wb\") as f:\n",
    "            self.client.download_fileobj(bucket, key, f)\n",
    "\n",
    "        return tmp_path\n",
    "\n",
    "    def upload_file(self, local_path: str, s3_uri: str):\n",
    "        \"\"\"Upload file to S3\"\"\"\n",
    "        bucket, key = self.parse_uri(s3_uri)\n",
    "        self.client.upload_file(local_path, bucket, key)\n",
    "        logger.info(f\"Uploaded to {s3_uri}\")\n",
    "\n",
    "class TranscriptionManager:\n",
    "    \"\"\"Manages multiple ASR pipelines for parallel processing\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.pipelines = []\n",
    "        self.pipeline_locks = []\n",
    "\n",
    "        num_workers = getattr(config, 'asr_workers', ASR_WORKERS)\n",
    "\n",
    "        # Create multiple pipeline instances\n",
    "        for i in range(num_workers):\n",
    "            if torch.cuda.is_available():\n",
    "                device_id = i % torch.cuda.device_count()\n",
    "            else:\n",
    "                device_id = -1\n",
    "\n",
    "            logger.info(f\"Creating ASR worker {i+1}/{num_workers} on device {device_id}\")\n",
    "            pipe = self._build_pipeline(device_id=device_id)\n",
    "            self.pipelines.append(pipe)\n",
    "            self.pipeline_locks.append(threading.Lock())\n",
    "\n",
    "    def _build_pipeline(self, device_id=-1):\n",
    "        \"\"\"Build the ASR pipeline\"\"\"\n",
    "        use_cuda = torch.cuda.is_available() and device_id >= 0\n",
    "        dtype = torch.float16 if use_cuda else torch.float32\n",
    "\n",
    "        generate_kwargs = {}\n",
    "        if self.config.language:\n",
    "            generate_kwargs[\"language\"] = self.config.language\n",
    "        if self.config.task:\n",
    "            generate_kwargs[\"task\"] = self.config.task\n",
    "\n",
    "        logger.info(f\"Loading ASR: {self.config.model_id} (device={device_id}, dtype={dtype})\")\n",
    "\n",
    "        return pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.config.model_id,\n",
    "            device=device_id,\n",
    "            torch_dtype=dtype,\n",
    "            return_timestamps=True,\n",
    "            chunk_length_s=self.config.chunk_length_s,\n",
    "            stride_length_s=self.config.stride_length_s,\n",
    "            generate_kwargs=generate_kwargs or None,\n",
    "            batch_size=BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "    def transcribe(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Single file transcription - uses first pipeline\"\"\"\n",
    "        return self._transcribe_with_pipeline(audio_path, 0)\n",
    "\n",
    "    def transcribe_batch_parallel(self, paths_and_keys: List[tuple]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Transcribe multiple files in parallel using ThreadPoolExecutor\"\"\"\n",
    "        num_workers = len(self.pipelines)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = []\n",
    "            for i, (path, key) in enumerate(paths_and_keys):\n",
    "                pipeline_idx = i % num_workers\n",
    "                future = executor.submit(\n",
    "                    self._transcribe_with_pipeline_and_key,\n",
    "                    path,\n",
    "                    key,\n",
    "                    pipeline_idx\n",
    "                )\n",
    "                futures.append(future)\n",
    "\n",
    "            # Collect results\n",
    "            results = []\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Transcription failed: {e}\")\n",
    "\n",
    "            return results\n",
    "\n",
    "    def _transcribe_with_pipeline(self, audio_path: str, pipeline_idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Transcribe using a specific pipeline instance\"\"\"\n",
    "        with self.pipeline_locks[pipeline_idx]:\n",
    "            try:\n",
    "                result = self.pipelines[pipeline_idx](audio_path)\n",
    "                text = result.get(\"text\", \"\") if isinstance(result, dict) else str(result)\n",
    "                return {\"text\": text, \"error\": None}\n",
    "            except Exception as e:\n",
    "                if \"ffmpeg\" in str(e).lower():\n",
    "                    return self._fallback_transcribe(audio_path, pipeline_idx)\n",
    "                return {\"text\": \"\", \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "\n",
    "    def _transcribe_with_pipeline_and_key(self, audio_path: str, s3_key: str, pipeline_idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Transcribe with key tracking for batch processing\"\"\"\n",
    "        result = self._transcribe_with_pipeline(audio_path, pipeline_idx)\n",
    "\n",
    "        if result[\"text\"]:\n",
    "            preview = result[\"text\"][:100] + \"...\" if len(result[\"text\"]) > 100 else result[\"text\"]\n",
    "            logger.info(f\"✓ Worker {pipeline_idx}: {os.path.basename(s3_key)}: {preview}\")\n",
    "        else:\n",
    "            logger.warning(f\"✗ Worker {pipeline_idx}: {os.path.basename(s3_key)}: {result['error']}\")\n",
    "\n",
    "        return {\n",
    "            \"s3_key\": s3_key,\n",
    "            \"filename\": os.path.basename(s3_key),\n",
    "            \"transcribed_text\": result[\"text\"],\n",
    "            \"error\": result[\"error\"] or \"\"\n",
    "        }\n",
    "\n",
    "    def _fallback_transcribe(self, audio_path: str, pipeline_idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback using torchaudio\"\"\"\n",
    "        try:\n",
    "            import torchaudio\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if waveform.ndim == 2:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "            with self.pipeline_locks[pipeline_idx]:\n",
    "                result = self.pipelines[pipeline_idx](waveform.squeeze(0).numpy(), sampling_rate=sr)\n",
    "                text = result.get(\"text\", \"\") if isinstance(result, dict) else str(result)\n",
    "                return {\"text\": text, \"error\": None}\n",
    "        except Exception as e:\n",
    "            return {\"text\": \"\", \"error\": f\"Fallback failed: {e}\"}\n",
    "\n",
    "    def cleanup_gpu_memory(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "class CSVManager:\n",
    "    \"\"\"Handles CSV operations \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "\n",
    "    def read_processed_keys(self) -> Set[str]:\n",
    "        \"\"\"Read already processed S3 keys from CSV\"\"\"\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            return set()\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path, usecols=[\"s3_key\"])\n",
    "            return set(df[\"s3_key\"].astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read existing CSV: {e}\")\n",
    "            return set()\n",
    "\n",
    "    def append_results(self, results: List[Dict[str, Any]]):\n",
    "        \"\"\"Append results to CSV\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        mode = \"a\" if os.path.exists(self.csv_path) else \"w\"\n",
    "        header = not os.path.exists(self.csv_path)\n",
    "\n",
    "        df.to_csv(self.csv_path, index=False, mode=mode, header=header)\n",
    "        logger.info(f\"Appended {len(results)} rows to {self.csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYIkt_eha5Wg"
   },
   "outputs": [],
   "source": [
    "class ValidationCSVManager:\n",
    "    \"\"\"Handles the CSV with validated files\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path: str, column_name: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def read_allowed_filenames(self) -> Set[str]:\n",
    "        \"\"\"Read the list of allowed filenames from the validation CSV\"\"\"\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            raise FileNotFoundError(f\"Validation CSV not found: {self.csv_path}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path, usecols=[self.column_name])\n",
    "            filenames = df[self.column_name].astype(str).tolist()\n",
    "            # Remove any NaN values and strip whitespace\n",
    "            filenames = [f.strip() for f in filenames if pd.notna(f) and f.strip()]\n",
    "            logger.info(f\"Loaded {len(filenames)} allowed filenames from {self.csv_path}\")\n",
    "            return set(filenames)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read validation CSV: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNEN__NQa5T4"
   },
   "outputs": [],
   "source": [
    "from threading import Lock\n",
    "\n",
    "class ProductionPipeline:\n",
    "    \"\"\"Main for the transcription pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.s3_manager = S3Manager(config)\n",
    "        self.transcription_manager = TranscriptionManager(config)\n",
    "        self.csv_manager = CSVManager(config.output_local_csv)\n",
    "        self.validation_csv_manager = ValidationCSVManager(\n",
    "            config.validation_csv_path,\n",
    "            config.validation_csv_column\n",
    "        )\n",
    "        self.results_buffer: List[Dict[str, Any]] = []\n",
    "        self.flush_lock = threading.Lock()\n",
    "\n",
    "    # unchanged run() ...\n",
    "    def run(self):\n",
    "        \"\"\"Execute the transcription pipeline\"\"\"\n",
    "        FFmpegSetup.ensure_available()\n",
    "\n",
    "        allowed_filenames = self.validation_csv_manager.read_allowed_filenames()\n",
    "        logger.info(f\"Will only process files from validation CSV: {len(allowed_filenames)} files\")\n",
    "\n",
    "        bucket, prefix = self.s3_manager.parse_uri(self.config.s3_input)\n",
    "        prefix_norm = prefix.rstrip(\"/\") + \"/\"\n",
    "\n",
    "        allowed_filenames = {os.path.basename(f.strip()) for f in allowed_filenames}\n",
    "        logger.info(f\"Will only process files from validation CSV (normalized): {len(allowed_filenames)}\")\n",
    "\n",
    "        existing = []\n",
    "        paginator = self.s3_manager.client.get_paginator(\"list_objects_v2\")\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix_norm):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(\"/\") or not self.s3_manager.is_audio_file(key):\n",
    "                    continue\n",
    "                if os.path.basename(key) in allowed_filenames:\n",
    "                    existing.append(key)\n",
    "\n",
    "        all_keys = existing\n",
    "        logger.info(f\"Found {len(all_keys)} matching files in S3\")\n",
    "\n",
    "        processed_keys = set()\n",
    "        if self.config.resume_from_csv:\n",
    "            processed_keys = self.csv_manager.read_processed_keys()\n",
    "\n",
    "        keys_to_process = [k for k in all_keys if k not in processed_keys]\n",
    "        logger.info(f\"Processing {len(keys_to_process)} files (skipped {len(processed_keys)} already done)\")\n",
    "\n",
    "        if not keys_to_process:\n",
    "            logger.info(\"No files to process\")\n",
    "            return\n",
    "\n",
    "        self._process_with_concurrency(bucket, keys_to_process)\n",
    "        self._flush_results()\n",
    "\n",
    "        if self.config.write_back_to_s3 and self.config.output_s3_uri:\n",
    "            self.s3_manager.upload_file(self.config.output_local_csv, self.config.output_s3_uri)\n",
    "\n",
    "        logger.info(\"Pipeline complete!\")\n",
    "\n",
    "    # unchanged helper methods up to _flush_results ...\n",
    "\n",
    "    def _flush_results(self):\n",
    "        \"\"\"Thread-safe flush to CSV\"\"\"\n",
    "        with self.flush_lock:\n",
    "            if not self.results_buffer:\n",
    "                return\n",
    "            try:\n",
    "                results_copy = [r for r in self.results_buffer if isinstance(r, dict)]\n",
    "                self.results_buffer.clear()\n",
    "                df = pd.DataFrame(results_copy)\n",
    "                mode = \"a\" if os.path.exists(self.config.output_local_csv) else \"w\"\n",
    "                header = not os.path.exists(self.config.output_local_csv)\n",
    "                df.to_csv(self.config.output_local_csv, index=False, mode=mode, header=header)\n",
    "                logger.info(f\"Appended {len(df)} rows to {self.config.output_local_csv}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during flush: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncProductionPipeline(ProductionPipeline):\n",
    "    \"\"\"Async variant of ProductionPipeline for concurrent processing\"\"\"\n",
    "\n",
    "    def _list_all_audio(self, bucket: str, prefix_norm: str, allowed_filenames: set[str]) -> list[str]:\n",
    "        \"\"\"List all matching audio keys from S3 (same logic as original run())\"\"\"\n",
    "        paginator = self.s3_manager.client.get_paginator(\"list_objects_v2\")\n",
    "        keys = []\n",
    "\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix_norm):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if key.endswith(\"/\"):\n",
    "                    continue\n",
    "                if not self.s3_manager.is_audio_file(key):\n",
    "                    continue\n",
    "                if os.path.basename(key) in allowed_filenames:\n",
    "                    keys.append(key)\n",
    "                    if self.config.max_files and len(keys) >= self.config.max_files:\n",
    "                        return keys\n",
    "\n",
    "        logger.info(f\"Using {len(keys)} keys from allowed list (after intersection)\")\n",
    "        return keys\n",
    "\n",
    "    def _filter_unprocessed(self, all_keys: list[str]) -> list[str]:\n",
    "        \"\"\"Filter out keys already present in output CSV\"\"\"\n",
    "        processed_keys = set()\n",
    "        if self.config.resume_from_csv:\n",
    "            processed_keys = self.csv_manager.read_processed_keys()\n",
    "\n",
    "        keys_to_process = [k for k in all_keys if k not in processed_keys]\n",
    "        logger.info(\n",
    "            f\"Processing {len(keys_to_process)} files \"\n",
    "            f\"(skipped {len(processed_keys)} already done)\"\n",
    "        )\n",
    "        return keys_to_process\n",
    "\n",
    "    async def _process_one(self, sem: asyncio.Semaphore, bucket: str, key: str, pipeline_idx: int):\n",
    "        \"\"\"One async unit of work: download → transcribe → cleanup\"\"\"\n",
    "        async with sem:\n",
    "            local_path = None\n",
    "            try:\n",
    "                local_path = await asyncio.to_thread(self.s3_manager.download_to_temp, bucket, key)\n",
    "                result = await asyncio.to_thread(\n",
    "                    self.transcription_manager._transcribe_with_pipeline_and_key,\n",
    "                    local_path, key, pipeline_idx\n",
    "                )\n",
    "                self.results_buffer.append(result)\n",
    "            except Exception as e:\n",
    "                self.results_buffer.append({\n",
    "                    \"s3_key\": key,\n",
    "                    \"filename\": os.path.basename(key),\n",
    "                    \"transcribed_text\": \"\",\n",
    "                    \"error\": f\"Error: {e}\"\n",
    "                })\n",
    "            finally:\n",
    "                if local_path and os.path.exists(local_path):\n",
    "                    try:\n",
    "                        os.remove(local_path)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            if len(self.results_buffer) >= self.config.append_every_n:\n",
    "                await asyncio.to_thread(self._flush_results)\n",
    "                self.transcription_manager.cleanup_gpu_memory()\n",
    "\n",
    "    async def _process_with_asyncio(self, bucket: str, keys: list[str], concurrency: int = 16):\n",
    "        \"\"\"Main async loop: orchestrate downloads + transcription concurrently\"\"\"\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "        tasks = [\n",
    "            self._process_one(sem, bucket, key, i % len(self.transcription_manager.pipelines))\n",
    "            for i, key in enumerate(keys)\n",
    "        ]\n",
    "\n",
    "        total = len(tasks)\n",
    "        start_time = datetime.now()\n",
    "        completed = 0\n",
    "\n",
    "        for fut in tqdm(asyncio.as_completed(tasks), total=total, desc=\"Processing files\"):\n",
    "            await fut\n",
    "            completed += 1\n",
    "            if completed % 10 == 0 or completed == total:\n",
    "                elapsed = (datetime.now() - start_time).total_seconds()\n",
    "                rate = completed / elapsed if elapsed > 0 else 0\n",
    "                remaining = total - completed\n",
    "                eta_min = (remaining / rate / 60) if rate > 0 else float(\"inf\")\n",
    "                logger.info(f\"{completed}/{total} done ({rate:.2f} files/s) — ETA {eta_min:.1f} min\")\n",
    "\n",
    "        await asyncio.to_thread(self._flush_results)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Override run() to use async version instead of _process_with_concurrency\"\"\"\n",
    "        FFmpegSetup.ensure_available()\n",
    "\n",
    "        allowed_filenames = self.validation_csv_manager.read_allowed_filenames()\n",
    "        allowed_filenames = {os.path.basename(f.strip()) for f in allowed_filenames}\n",
    "        logger.info(f\"Will only process files from validation CSV: {len(allowed_filenames)}\")\n",
    "\n",
    "        bucket, prefix = self.s3_manager.parse_uri(self.config.s3_input)\n",
    "        prefix_norm = prefix.rstrip(\"/\") + \"/\"\n",
    "\n",
    "        all_keys = self._list_all_audio(bucket, prefix_norm, allowed_filenames)\n",
    "        keys_to_process = self._filter_unprocessed(all_keys)\n",
    "        if not keys_to_process:\n",
    "            logger.info(\"No files to process.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting async transcription on {len(keys_to_process)} files...\")\n",
    "\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                task = loop.create_task(self._process_with_asyncio(bucket, keys_to_process))\n",
    "                loop.run_until_complete(task)\n",
    "            else:\n",
    "                asyncio.run(self._process_with_asyncio(bucket, keys_to_process))\n",
    "        except RuntimeError:\n",
    "            asyncio.get_event_loop().run_until_complete(self._process_with_asyncio(bucket, keys_to_process))\n",
    "\n",
    "        if self.config.write_back_to_s3 and self.config.output_s3_uri:\n",
    "            self.s3_manager.upload_file(self.config.output_local_csv, self.config.output_s3_uri)\n",
    "\n",
    "        logger.info(\"Async pipeline complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rs-rx9Uka5Rw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ======================================== Run main loop ========================================\n",
    "def main():\n",
    "    try:\n",
    "        config = Config()\n",
    "        # pipeline = ProductionPipeline(config)\n",
    "        pipeline = AsyncProductionPipeline(config)\n",
    "        pipeline.run()\n",
    "        return 0\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Interrupted by user\")\n",
    "        return 130\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    if result == 0:\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(f\"Pipeline failed with code: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
