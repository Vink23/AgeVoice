{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de478ce8-a585-4cdd-a39a-05499332d314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:38:19.729953Z",
     "iopub.status.busy": "2025-10-13T23:38:19.729703Z",
     "iopub.status.idle": "2025-10-13T23:38:21.446016Z",
     "shell.execute_reply": "2025-10-13T23:38:21.445244Z",
     "shell.execute_reply.started": "2025-10-13T23:38:19.729932Z"
    },
    "id": "de478ce8-a585-4cdd-a39a-05499332d314"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet sagemaker jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647ce642-1577-4bf1-8153-1eb69adb6887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:38:21.447274Z",
     "iopub.status.busy": "2025-10-13T23:38:21.447004Z",
     "iopub.status.idle": "2025-10-13T23:38:24.061582Z",
     "shell.execute_reply": "2025-10-13T23:38:24.060976Z",
     "shell.execute_reply.started": "2025-10-13T23:38:21.447248Z"
    },
    "id": "647ce642-1577-4bf1-8153-1eb69adb6887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::867344443757:role/service-role/AmazonSageMaker-ExecutionRole-20250917T175306\n",
      "sagemaker bucket: asrelder-data\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os, json, pandas as pd\n",
    "import csv\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader, s3_path_join\n",
    "import boto3\n",
    "import time, sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket= \"asrelder-data\"\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6fd2d18-1324-4618-87bc-c41fd518f0c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:44:39.993051Z",
     "iopub.status.busy": "2025-10-13T23:44:39.992791Z",
     "iopub.status.idle": "2025-10-13T23:44:40.605785Z",
     "shell.execute_reply": "2025-10-13T23:44:40.605195Z",
     "shell.execute_reply.started": "2025-10-13T23:44:39.993029Z"
    },
    "id": "a6fd2d18-1324-4618-87bc-c41fd518f0c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://asrelder-data/common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\n",
      "asrelder-data\n",
      "common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\n",
      "Listing audio files from s3://asrelder-data/common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/...\n"
     ]
    }
   ],
   "source": [
    "audio_files_s3_path = \"s3://asrelder-data/common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\"\n",
    "bucket_name = audio_files_s3_path.replace(\"s3://\", \"\").split(\"/\")[0]\n",
    "prefix = \"/\".join(audio_files_s3_path.replace(\"s3://\", \"\").split(\"/\")[1:])\n",
    "input_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"whisper_batch/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"whisper_batch/output\")\n",
    "s3_client = boto3.client('s3')\n",
    "print(audio_files_s3_path)\n",
    "print(bucket_name)\n",
    "print(prefix)\n",
    "print(f\"Listing audio files from {audio_files_s3_path}...\")\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85f17e-6fde-4cdd-a086-dcc6f0fe7f2e",
   "metadata": {
    "id": "9f85f17e-6fde-4cdd-a086-dcc6f0fe7f2e"
   },
   "outputs": [],
   "source": [
    "# Manifest code --- Uncomment if we need Manifest\n",
    "\n",
    "\n",
    "# Load validation CSV\n",
    "# validation_df = pd.read_csv(\"common_voices_23_train_with_validated_votes.csv\")\n",
    "# print(f\"Validation CSV has {len(validation_df)} entries\")\n",
    "\n",
    "# # Extract filenames from the 'path' column and create a set for fast lookup\n",
    "# valid_filenames = set(validation_df['path'].values)\n",
    "# print(f\"Found {len(valid_filenames)} unique filenames in validation CSV\")\n",
    "\n",
    "# # Filter audio_files to only include validated files\n",
    "# filtered_audio_files = []\n",
    "# for audio in tqdm(audio_files, desc=\"Filtering files\"):\n",
    "#     if audio['filename'] in valid_filenames:\n",
    "#         filtered_audio_files.append(audio)\n",
    "\n",
    "# print(f\"\\nFiltered: {len(filtered_audio_files)} files matched (out of {len(audio_files)} total)\")\n",
    "\n",
    "# # Replace audio_files with filtered version\n",
    "# audio_files = filtered_audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ed1f3-6e2f-4bc4-980d-e671c6502cd8",
   "metadata": {
    "id": "202ed1f3-6e2f-4bc4-980d-e671c6502cd8"
   },
   "outputs": [],
   "source": [
    "# Manifest code --- Uncomment if we need Manifest\n",
    "\n",
    "# CLIPS_PREFIX = \"s3://asrelder-data/common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\"\n",
    "# df = pd.read_csv(\"common_voices_23_train_with_validated_votes.csv\", usecols=[\"path\"])\n",
    "\n",
    "# names = df[\"path\"].astype(str).apply(lambda p: os.path.basename(p.strip()))\n",
    "\n",
    "# manifest_path = \"validated_manifest.jsonl\"\n",
    "# with open(manifest_path, \"w\") as f:\n",
    "#     for n in names:\n",
    "#         f.write(json.dumps({\"source-ref\": CLIPS_PREFIX + n}) + \"\\n\")\n",
    "\n",
    "# manifest_s3_uri = S3Uploader.upload(manifest_path, \"s3://asrelder-data/whisper_batch/input/\")\n",
    "# print(\"Manifest:\", manifest_s3_uri, \"Lines:\", len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f147f2-faa0-4978-8053-dd22b4092459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:45:17.877376Z",
     "iopub.status.busy": "2025-10-13T23:45:17.877109Z",
     "iopub.status.idle": "2025-10-13T23:45:18.763520Z",
     "shell.execute_reply": "2025-10-13T23:45:18.762947Z",
     "shell.execute_reply.started": "2025-10-13T23:45:17.877353Z"
    },
    "id": "02f147f2-faa0-4978-8053-dd22b4092459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: whisper-asr-1760399117\n"
     ]
    }
   ],
   "source": [
    "env = {\n",
    "    \"HF_MODEL_ID\": \"openai/whisper-base\",\n",
    "    \"HF_TASK\": \"automatic-speech-recognition\",\n",
    "    \"SAGEMAKER_PROGRAM\": \"inference.py\",  # <- critical\n",
    "}\n",
    "\n",
    "model_name = f\"whisper-asr-{int(time.time())}\"\n",
    "\n",
    "hf_model = HuggingFaceModel(\n",
    "    env=env,\n",
    "    role=role,\n",
    "    transformers_version=\"4.26\",\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py39\",\n",
    "    source_dir=\"Untitled/code\",\n",
    "    entry_point=\"inference.py\",\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "# Create a new transformer from this model\n",
    "output_s3_path = \"s3://asrelder-data/whisper_batch/output/dementiabank\"\n",
    "batch = hf_model.transformer(\n",
    "    instance_count=3,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    output_path=output_s3_path,\n",
    "    strategy=\"SingleRecord\",\n",
    "    accept=\"application/json\",\n",
    "    #assemble_with=\"Line\",\n",
    "    max_payload=50,\n",
    "    max_concurrent_transforms=1\n",
    ")\n",
    "print(\"Using model:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c348fb-210d-4e66-b629-4318216d56f2",
   "metadata": {
    "id": "43c348fb-210d-4e66-b629-4318216d56f2"
   },
   "outputs": [],
   "source": [
    "# Build the CV manifest --- Uncomment to make manifest\n",
    "\n",
    "\n",
    "\n",
    "# prefix = \"s3://asrelder-data/common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\"\n",
    "# bucket = \"asrelder-data\"\n",
    "# key_prefix = \"common_voice/23/cv-corpus-23.0-2025-09-05/en/clips/\"\n",
    "\n",
    "# # list all .mp3 under the prefix (paginate-safe)\n",
    "# files = []\n",
    "# kwargs = dict(Bucket=bucket, Prefix=key_prefix)\n",
    "# while True:\n",
    "#     resp = s3.list_objects_v2(**kwargs)\n",
    "#     for obj in resp.get(\"Contents\", []):\n",
    "#         if obj[\"Key\"].endswith(\".mp3\"):\n",
    "#             files.append(obj[\"Key\"].replace(key_prefix, \"\"))\n",
    "#     if resp.get(\"IsTruncated\"):\n",
    "#         kwargs[\"ContinuationToken\"] = resp[\"NextContinuationToken\"]\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# manifest = [{\"prefix\": prefix}]\n",
    "# manifest.extend(files)\n",
    "\n",
    "# # write a one-line JSON array\n",
    "# local_manifest = \"manifest.json\"\n",
    "# with jsonlines.open(local_manifest, mode=\"w\") as w:\n",
    "#     w.write(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aae7055-92ed-4e73-ba5d-a7961a2609bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:40:22.432963Z",
     "iopub.status.busy": "2025-10-13T23:40:22.432703Z",
     "iopub.status.idle": "2025-10-13T23:40:22.631217Z",
     "shell.execute_reply": "2025-10-13T23:40:22.630622Z",
     "shell.execute_reply.started": "2025-10-13T23:40:22.432943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the Dementia manifest --- Uncomment to make manifest\n",
    "\n",
    "# prefix = \"s3://asrelder-data/dementiabank/\"\n",
    "# bucket = \"asrelder-data\"\n",
    "# key_prefix = \"dementiabank/\"\n",
    "\n",
    "# # Make the S3 client\n",
    "# s3 = boto3.client(\"s3\")\n",
    "\n",
    "# # List all .mp3/.wav with a paginator\n",
    "# files = []\n",
    "# paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "# for page in paginator.paginate(Bucket=bucket, Prefix=key_prefix):\n",
    "#     for obj in page.get(\"Contents\", []):\n",
    "#         key = obj[\"Key\"]\n",
    "#         if key.endswith((\".mp3\", \".wav\")):  # tuple, not list\n",
    "#             # strip only the leading prefix\n",
    "#             files.append(key[len(key_prefix):])\n",
    "\n",
    "# manifest_as_array = [{\"prefix\": prefix}, *files]\n",
    "# with open(\"dementia_manifest.json\", \"w\") as f:\n",
    "#     json.dump(manifest_as_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca2bedd-fe70-41fc-a2aa-82325148d5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:43:38.667431Z",
     "iopub.status.busy": "2025-10-13T23:43:38.667177Z",
     "iopub.status.idle": "2025-10-13T23:43:38.810190Z",
     "shell.execute_reply": "2025-10-13T23:43:38.809628Z",
     "shell.execute_reply.started": "2025-10-13T23:43:38.667412Z"
    },
    "id": "bca2bedd-fe70-41fc-a2aa-82325148d5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest at: s3://asrelder-data/whisper_batch/input/dementia_manifest.json\n"
     ]
    }
   ],
   "source": [
    "#Maniest Code --- Uncomment to check manifest\n",
    "\n",
    "# sess = sagemaker.Session()\n",
    "\n",
    "# bucket = \"asrelder-data\"\n",
    "# local_manifest = \"dementia_manifest.json\"  \n",
    "\n",
    "# # sanity check\n",
    "# if not os.path.exists(local_manifest):\n",
    "#     raise FileNotFoundError(f\"Missing file: {local_manifest}\")\n",
    "\n",
    "# dementia_manifest_s3 = sess.upload_data(\n",
    "#     path=local_manifest,\n",
    "#     bucket=bucket,\n",
    "#     key_prefix=\"whisper_batch/input\" \n",
    "# )\n",
    "# print(\"Manifest at:\", dementia_manifest_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c5c31c8-8e06-44d0-a6d8-1cf6f0768afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:45:22.903131Z",
     "iopub.status.busy": "2025-10-13T23:45:22.902874Z",
     "iopub.status.idle": "2025-10-13T23:45:22.905954Z",
     "shell.execute_reply": "2025-10-13T23:45:22.905398Z",
     "shell.execute_reply.started": "2025-10-13T23:45:22.903110Z"
    }
   },
   "outputs": [],
   "source": [
    "dementia_manifest_s3 = \"s3://asrelder-data/whisper_batch/input/dementia_manifest.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a35a624-2e35-48f4-b203-ea25e3176f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T23:45:24.418118Z",
     "iopub.status.busy": "2025-10-13T23:45:24.417865Z",
     "iopub.status.idle": "2025-10-13T23:51:59.994581Z",
     "shell.execute_reply": "2025-10-13T23:51:59.994027Z",
     "shell.execute_reply.started": "2025-10-13T23:45:24.418098Z"
    },
    "id": "5a35a624-2e35-48f4-b203-ea25e3176f7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: whisper-asr-1760399117-2025-10-13-23-45-24-419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................\u001b[32mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[32mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.62it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:01<00:01,  3.50it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:01<00:00,  6.66it/s]\u001b[0m\n",
      "\u001b[32mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[32mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[32mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[34mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.62it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:01<00:01,  3.50it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:01<00:00,  6.66it/s]\u001b[0m\n",
      "\u001b[34mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[34mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,882 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[32mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mCurrent directory: /\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:13,882 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[34mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[32mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[32mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[32mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[32mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[32mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[32mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[32mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[32mInitial Models: ALL\u001b[0m\n",
      "\u001b[32mLog dir: null\u001b[0m\n",
      "\u001b[32mMetrics dir: null\u001b[0m\n",
      "\u001b[32mNetty threads: 0\u001b[0m\n",
      "\u001b[32mNetty client threads: 0\u001b[0m\n",
      "\u001b[32mDefault workers per model: 1\u001b[0m\n",
      "\u001b[32mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[32mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[32mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[32mPreload model: false\u001b[0m\n",
      "\u001b[32mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,948 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,035 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,037 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,039 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[34mInitial Models: ALL\u001b[0m\n",
      "\u001b[34mLog dir: null\u001b[0m\n",
      "\u001b[34mMetrics dir: null\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[34mPreload model: false\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:13,948 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,035 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,037 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,039 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,044 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,053 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,130 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel server started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,134 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,141 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,044 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,053 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,130 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,134 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,141 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[32mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[32mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.14it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:01<00:01,  5.90it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:01<00:00, 11.04it/s]\u001b[0m\n",
      "\u001b[32mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[32mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[32mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,395 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[32mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mCurrent directory: /\u001b[0m\n",
      "\u001b[32mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[32mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[32mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[32mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[32mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[32mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[32mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[32mInitial Models: ALL\u001b[0m\n",
      "\u001b[32mLog dir: null\u001b[0m\n",
      "\u001b[32mMetrics dir: null\u001b[0m\n",
      "\u001b[32mNetty threads: 0\u001b[0m\n",
      "\u001b[32mNetty client threads: 0\u001b[0m\n",
      "\u001b[32mDefault workers per model: 1\u001b[0m\n",
      "\u001b[32mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[32mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[32mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[32mPreload model: false\u001b[0m\n",
      "\u001b[32mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,458 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,537 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,539 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,539 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,539 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,540 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,540 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,547 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,554 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,621 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel server started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,624 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,631 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,763 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:59356 \"GET /ping HTTP/1.1\" 200 24\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,781 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:59362 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2025-10-13T23:51:15.510:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[33m2025-10-13T23:51:15.510:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[34mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[34mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.02it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:02<00:02,  2.96it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:02<00:00,  5.64it/s]\u001b[0m\n",
      "\u001b[34mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[34mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,912 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[34mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[36mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[36m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[36mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[36m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.02it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:02<00:02,  2.96it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:02<00:00,  5.64it/s]\u001b[0m\n",
      "\u001b[36mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[36mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[36mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:14,912 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[36mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[34mInitial Models: ALL\u001b[0m\n",
      "\u001b[34mLog dir: null\u001b[0m\n",
      "\u001b[34mMetrics dir: null\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[34mPreload model: false\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,984 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,064 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,067 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[36mCurrent directory: /\u001b[0m\n",
      "\u001b[36mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[36mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[36mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[36mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[36mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[36mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[36mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[36mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[36mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[36mInitial Models: ALL\u001b[0m\n",
      "\u001b[36mLog dir: null\u001b[0m\n",
      "\u001b[36mMetrics dir: null\u001b[0m\n",
      "\u001b[36mNetty threads: 0\u001b[0m\n",
      "\u001b[36mNetty client threads: 0\u001b[0m\n",
      "\u001b[36mDefault workers per model: 1\u001b[0m\n",
      "\u001b[36mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[36mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[36mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[36mPreload model: false\u001b[0m\n",
      "\u001b[36mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:14,984 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,064 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,067 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,069 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,074 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,088 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,161 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,169 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,190 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,474 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:47780 \"GET /ping HTTP/1.1\" 200 13\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,504 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47794 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,069 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,074 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,088 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,161 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[36mModel server started.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,169 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,190 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,474 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:47780 \"GET /ping HTTP/1.1\" 200 13\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,504 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47794 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,020 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:39048 \"GET /ping HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,039 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:39058 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:16,020 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:39048 \"GET /ping HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:16,039 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:39058 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,173 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,383 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:16,383 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:17,472 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:17,472 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[35m2025-10-13T23:51:16.049:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:14.803:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,002 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000021-00000001-a8eebc9d8dfc3b99-7b61a5f6\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,004 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5311\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,005 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:20,222 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000023-00000001-703e2bc48dfc419b-bc5041a9\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:20,223 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5013\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:20,224 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:20,222 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000023-00000001-703e2bc48dfc419b-bc5041a9\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:20,223 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5013\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:20,224 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,508 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000021-00000001-d5a840a78dfc3d8d-6ceaa5ad\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5320\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,511 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:19,508 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000021-00000001-d5a840a78dfc3d8d-6ceaa5ad\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:19,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5320\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:19,511 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,140 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,141 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,239 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08058547973632812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1726\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 4514\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,240 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1724.715232849121 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,241 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08225440979003906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,140 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,141 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,239 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08058547973632812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1726\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 4514\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,240 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1724.715232849121 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,241 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08225440979003906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,604 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,605 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,685 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07843971252441406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,686 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1677.2093772888184 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,686 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1679\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,686 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07748603820800781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,687 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 5651\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,958 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,959 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,058 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12803077697753906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 295\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 298\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 293.4834957122803 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,253 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,253 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04458427429199219 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,307 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 233\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 232.2559356689453 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,308 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,308 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06198883056640625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,509 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,586 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06985664367675781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,586 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 282\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,587 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.0986042022705 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.16546249389648438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 328.7193775177002 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 330\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,950 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 334\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,285 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5092620849609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,586 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06985664367675781 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,586 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 282\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,587 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.0986042022705 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.16546249389648438 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 328.7193775177002 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 330\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,950 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 334\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,285 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5092620849609375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 311.5224838256836 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,286 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 311.5224838256836 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,286 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,595 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.36215782165527344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,595 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 267\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,596 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 271\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,596 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 265.71154594421387 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,596 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06198883056640625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,793 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,870 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.23627281188964844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,870 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.0976276397705 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,870 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,871 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06198883056640625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,871 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 259\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,078 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,078 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,188 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.17690658569335938 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,188 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 293.0786609649658 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,189 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 294\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,189 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 298\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,386 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,386 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,456 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12183189392089844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,456 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.22857093811035 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,456 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,457 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,457 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,490 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,491 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1220703125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 290\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 288.1276607513428 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,595 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 293\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,922 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12111663818359375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 312.7858638763428 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,922 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,923 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 317\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,490 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,491 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1220703125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 290\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 288.1276607513428 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,595 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 293\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,922 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12111663818359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 312.7858638763428 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,922 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,923 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 317\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.14162063598632812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 349\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 347.83411026000977 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 352\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.14162063598632812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 349\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 347.83411026000977 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 352\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,669 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,670 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,824 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11396408081054688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,824 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 353\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,824 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 352.68449783325195 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,825 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 357\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,825 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,033 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,034 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,172 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8375644683837891 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,172 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 326\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,172 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 323.9307403564453 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,173 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 331\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,173 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,430 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,430 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,482 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,483 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.091552734375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.9881420135498 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,482 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,483 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.091552734375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.9881420135498 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12040138244628906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.7273750305176 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 271\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,090 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,091 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4343986511230469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 458.73451232910156 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 460\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,344 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 472\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,363 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,364 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12040138244628906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.7273750305176 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 271\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,090 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,091 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4343986511230469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 458.73451232910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 460\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,344 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 472\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,363 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,364 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\n",
      "\u001b[34m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1682.9359531402588 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,911 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1684\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07867813110351562 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,912 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 5747\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1682.9359531402588 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,911 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1684\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07867813110351562 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,912 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 5747\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,295 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11277198791503906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 314.8057460784912 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 316\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3116130828857422 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.2490348815918 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,856 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.24056434631347656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 250\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 248.3346462249756 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10323524475097656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.1739845275879 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 265\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,295 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11277198791503906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 314.8057460784912 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 316\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3116130828857422 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.2490348815918 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,856 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.24056434631347656 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 250\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 248.3346462249756 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10323524475097656 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.1739845275879 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 265\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 268\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 268\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,341 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,342 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4444122314453125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 290.75098037719727 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,458 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 292\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,459 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,459 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 296\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,840 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.40459632873535156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 368\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 365.7352924346924 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 370\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,040 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,041 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,186 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,186 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 326\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 325.84142684936523 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,187 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 331\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,341 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,342 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4444122314453125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 290.75098037719727 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,458 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 292\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,459 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,459 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 296\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,840 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.40459632873535156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 368\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 365.7352924346924 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 370\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,040 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,041 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,186 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,186 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 326\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 325.84142684936523 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,187 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 331\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.24247169494628906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 365\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 364.0313148498535 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 369\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,575 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,576 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,756 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,756 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5209445953369141 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 303.7891387939453 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 305\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,880 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 310\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,899 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,899 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.45561790466308594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 304\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 302.69503593444824 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,203 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 311\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,220 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,220 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,402 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,402 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5242824554443359 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 287\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 286.0274314880371 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 291\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,507 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,521 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07486343383789062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.7702579498291 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06771087646484375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 266\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.9803161621094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 270\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.29850006103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,074 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 320\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 319.06723976135254 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 325\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07486343383789062 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.7702579498291 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06771087646484375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 266\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.9803161621094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 270\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.29850006103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,074 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 320\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 319.06723976135254 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 325\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,703 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,703 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,780 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3192424774169922 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.8334083557129 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 264\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,793 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,793 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,983 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,983 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,098 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3936290740966797 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,098 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 306\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,098 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 304.4602870941162 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,099 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.18286705017089844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,099 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 309\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,113 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,113 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,293 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,293 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,382 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.29730796813964844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 271\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 269.6421146392822 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 274\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.23174285888671875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 301.8605709075928 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 304\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 308\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8287429809570312 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 306\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 303.7574291229248 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 313\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,740 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,741 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,915 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,916 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2791881561279297 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 246.70004844665527 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 248\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,988 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,170 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,171 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,216 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.20647048950195312 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 220\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 218.7364101409912 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 221\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,251 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,252 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.23174285888671875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 301.8605709075928 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 304\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 308\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8287429809570312 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 306\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 303.7574291229248 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 313\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,740 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,741 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,915 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,916 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2791881561279297 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 246.70004844665527 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 248\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,988 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,170 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,171 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,216 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.20647048950195312 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 220\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 218.7364101409912 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 221\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,251 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,252 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.7693767547607422 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.3625240325928 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,753 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.7693767547607422 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.3625240325928 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,753 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,754 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5369186401367188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.7336139678955 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,817 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,069 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07605552673339844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,069 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 243\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,070 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.26784706115723 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,086 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,754 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5369186401367188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.7336139678955 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,817 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,069 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07605552673339844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,069 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 243\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,070 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.26784706115723 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,086 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,607 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,607 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,767 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.27942657470703125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,767 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 358\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,767 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 356.67967796325684 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,768 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06985664367675781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,768 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 367\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,784 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,784 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,970 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,970 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5540847778320312 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 277.0516872406006 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,062 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 283\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,081 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,082 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,269 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,270 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.7097721099853516 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 259\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 257.4782371520996 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,340 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,350 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,351 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,535 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,535 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.45800209045410156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 341.40968322753906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 343\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 354\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1659393310546875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.27847290039062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12087821960449219 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 245.70322036743164 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05888938903808594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 249\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.45800209045410156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 341.40968322753906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 343\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 354\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1659393310546875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.27847290039062 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12087821960449219 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 245.70322036743164 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05888938903808594 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 249\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1838207244873047 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 298\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 296.3528633117676 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,384 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1838207244873047 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 298\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 296.3528633117676 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,384 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,615 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,616 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8833408355712891 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 335\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 332.95488357543945 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 343\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,747 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,758 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,943 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,944 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2512931823730469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 258\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.07545471191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,015 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1666545867919922 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 283\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,308 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.6140651702881 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,309 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,320 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,615 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,616 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8833408355712891 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 335\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 332.95488357543945 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 343\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,747 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,758 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,943 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,944 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2512931823730469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 258\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.07545471191406 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,015 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1666545867919922 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 283\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.6140651702881 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,309 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,320 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,321 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,321 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,584 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04792213439941406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,584 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 233.66951942443848 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,584 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,585 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07390975952148438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,585 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 237\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,601 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,601 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,786 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,786 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.27060508728027344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 283.9467525482178 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 285\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,886 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 289\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,896 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,897 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,072 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,073 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.0743865966796875 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 233.02865028381348 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,145 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,145 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,330 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,331 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,432 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.21982192993164062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,432 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 288\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,432 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 286.83924674987793 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,433 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,433 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 292\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,440 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,440 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,307 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,307 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,308 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1575946807861328 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.39399337768555 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,417 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,418 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,602 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,603 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2923011779785156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 276.55887603759766 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06389617919921875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,695 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 283\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,706 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,707 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,978 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09202957153320312 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 272.1548080444336 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 273\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 275\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,228 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.021457672119140625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.38245391845703 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 245\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1575946807861328 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.39399337768555 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,417 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,418 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,602 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,603 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2923011779785156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 276.55887603759766 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06389617919921875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,695 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 283\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,706 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,707 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,978 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09202957153320312 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 272.1548080444336 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 273\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 275\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,228 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.021457672119140625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.38245391845703 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 245\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,530 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,531 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.34499168395996094 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 276\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 273.68783950805664 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,829 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,830 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 292.675256729126 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 294\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,902 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,084 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.023365020751953125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.49729919433594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,530 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,531 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.34499168395996094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 276\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 273.68783950805664 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,829 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,830 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 292.675256729126 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 294\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,902 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,084 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.023365020751953125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.49729919433594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,610 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,610 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,667 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.03552436828613281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 229\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 227.4479866027832 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 230\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,678 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,678 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,855 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,855 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,932 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.024318695068359375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 254.28128242492676 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,950 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,950 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,132 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,133 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1609325408935547 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.91807746887207 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,213 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,214 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,392 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,392 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07557868957519531 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 273.2107639312744 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 275\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 277\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,497 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,497 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,409 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2384185791015625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 232.46383666992188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.058650970458984375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,473 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.025510787963867188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.01561164855957 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,409 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2384185791015625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 232.46383666992188 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.058650970458984375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,473 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.025510787963867188 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.01561164855957 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08130073547363281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 256.0882568359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,002 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 259\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.26607513427734375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.29400062561035 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08130073547363281 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 256.0882568359375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,002 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 259\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.26607513427734375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.29400062561035 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07462501525878906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.764741897583 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06651878356933594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 239\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,583 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,584 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02956390380859375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.7834014892578 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07462501525878906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.764741897583 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06651878356933594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 239\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,583 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,584 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02956390380859375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.7834014892578 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.015020370483398438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.87924575805664 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,909 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 248\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,109 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,110 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.22363662719726562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 285.28714179992676 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 286\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06628036499023438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 290\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.015020370483398438 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.87924575805664 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,909 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 248\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,109 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,110 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.22363662719726562 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 285.28714179992676 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 286\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06628036499023438 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 290\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,672 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,673 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3325939178466797 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 237.379789352417 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 239\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 241\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,746 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,746 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,923 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,923 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.35881996154785156 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.74362754821777 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,011 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,011 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,188 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,188 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.31185150146484375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.07211685180664 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,267 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,283 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,283 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,462 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,462 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,509 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13756752014160156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 244\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.3837184906006 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,693 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,694 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10085105895996094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 277.1868705749512 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0591278076171875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 281\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,980 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,981 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,051 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02384185791015625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.9655532836914 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,509 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13756752014160156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 244\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.3837184906006 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,693 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,694 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10085105895996094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 277.1868705749512 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0591278076171875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 281\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,980 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,981 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,051 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02384185791015625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.9655532836914 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4849433898925781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 309.950590133667 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 312\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06842613220214844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,534 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 314\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.48613548278808594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.49539375305176 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.051975250244140625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 251\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 249.42398071289062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,268 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,269 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,338 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11539459228515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.93698501586914 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08511543273925781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4849433898925781 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 309.950590133667 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 312\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06842613220214844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,534 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 314\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.48613548278808594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.49539375305176 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.051975250244140625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 251\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 249.42398071289062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,268 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,269 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,338 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11539459228515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.93698501586914 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08511543273925781 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,570 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13589859008789062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 289\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 287.6467704772949 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 291\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,582 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,582 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1404285430908203 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 248\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 246.5190887451172 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06222724914550781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 250\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,845 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,845 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,025 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.19431114196777344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 295.8817481994629 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 300\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,156 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,156 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,338 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,338 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.15592575073242188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 317.3072338104248 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 318\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,474 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 323\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,484 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,484 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07414817810058594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.460636138916 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,328 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,574 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2357959747314453 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 236\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.11481285095215 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07414817810058594 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.460636138916 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,328 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,574 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2357959747314453 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 236\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.11481285095215 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 247.99561500549316 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 249\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,834 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 251\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2033710479736328 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.48221588134766 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 247.99561500549316 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 249\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,834 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 251\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2033710479736328 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.48221588134766 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,559 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,560 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,643 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1201629638671875 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.67771339416504 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,917 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.85486602783203 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,918 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,918 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,926 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,927 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,190 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04887580871582031 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.655424118042 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,559 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,560 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,643 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1201629638671875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.67771339416504 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,917 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.85486602783203 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,918 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,918 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,926 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,927 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,190 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04887580871582031 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.655424118042 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,657 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,722 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04982948303222656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,722 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 239\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,722 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 238.0218505859375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,723 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 241\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,730 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,730 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,905 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,905 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04935264587402344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.45176124572754 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,999 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,186 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,186 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.16736984252929688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 297.98316955566406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06628036499023438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 299\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 304\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,306 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,306 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,480 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,480 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,270 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,271 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,270 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,271 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06937980651855469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 257.8301429748535 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,347 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,346 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 258\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,347 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 261\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,607 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.05269050598144531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.55432319641113 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 253\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.15091896057128906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,879 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.2662410736084 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,880 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4470348358154297 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 295.4273223876953 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 300\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06937980651855469 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 257.8301429748535 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,347 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,346 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 258\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,347 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 261\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,607 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.05269050598144531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.55432319641113 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 253\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.15091896057128906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,879 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.2662410736084 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,880 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4470348358154297 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 297\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 295.4273223876953 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 300\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04982948303222656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.0991096496582 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 274.2452621459961 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 275\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13327598571777344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 299\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 297.41835594177246 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,044 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,263 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,264 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2689361572265625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 322.7851390838623 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04982948303222656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.0991096496582 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 274.2452621459961 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 275\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13327598571777344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 299\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 297.41835594177246 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,044 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,263 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,264 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2689361572265625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 322.7851390838623 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 324\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 328\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 324\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 328\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,570 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04649162292480469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,570 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.5101547241211 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,571 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,571 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 265\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,571 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 268\u001b[0m\n",
      "\u001b[32mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[32mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.62it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:01<00:01,  3.50it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:01<00:00,  6.66it/s]\u001b[0m\n",
      "\u001b[32mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[32mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[32mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[34mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.62it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:01<00:01,  3.50it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:01<00:00,  6.66it/s]\u001b[0m\n",
      "\u001b[34mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[34mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,882 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[32mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mCurrent directory: /\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:13,882 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[34mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[32mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[32mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[32mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[32mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[32mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[32mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[32mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[32mInitial Models: ALL\u001b[0m\n",
      "\u001b[32mLog dir: null\u001b[0m\n",
      "\u001b[32mMetrics dir: null\u001b[0m\n",
      "\u001b[32mNetty threads: 0\u001b[0m\n",
      "\u001b[32mNetty client threads: 0\u001b[0m\n",
      "\u001b[32mDefault workers per model: 1\u001b[0m\n",
      "\u001b[32mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[32mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[32mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[32mPreload model: false\u001b[0m\n",
      "\u001b[32mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,948 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,035 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,037 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,039 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[34mInitial Models: ALL\u001b[0m\n",
      "\u001b[34mLog dir: null\u001b[0m\n",
      "\u001b[34mMetrics dir: null\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[34mPreload model: false\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:13,948 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,035 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,037 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,038 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,039 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,044 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,053 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,130 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel server started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,134 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,141 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,044 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,053 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,130 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,134 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,141 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[32mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[32m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[32mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[32m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.14it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:01<00:01,  5.90it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:01<00:00, 11.04it/s]\u001b[0m\n",
      "\u001b[32mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[32mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[32mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,395 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[32mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mCurrent directory: /\u001b[0m\n",
      "\u001b[32mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[32mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[32mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[32mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[32mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[32mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[32mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[32mInitial Models: ALL\u001b[0m\n",
      "\u001b[32mLog dir: null\u001b[0m\n",
      "\u001b[32mMetrics dir: null\u001b[0m\n",
      "\u001b[32mNetty threads: 0\u001b[0m\n",
      "\u001b[32mNetty client threads: 0\u001b[0m\n",
      "\u001b[32mDefault workers per model: 1\u001b[0m\n",
      "\u001b[32mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[32mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[32mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[32mPreload model: false\u001b[0m\n",
      "\u001b[32mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,458 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,537 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,539 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,539 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,539 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,540 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,540 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,547 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,554 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,621 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[32mModel server started.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,624 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:13,631 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,763 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:59356 \"GET /ping HTTP/1.1\" 200 24\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:14,781 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:59362 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2025-10-13T23:51:15.510:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[33m2025-10-13T23:51:15.510:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[34mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[34mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.02it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:02<00:02,  2.96it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:02<00:00,  5.64it/s]\u001b[0m\n",
      "\u001b[34mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[34mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,912 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[34mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[36mThis is an experimental beta features, which allows downloading model from the Hugging Face Hub on start up. It loads the model defined in the env var `HF_MODEL_ID`\u001b[0m\n",
      "\u001b[36m/opt/conda/lib/python3.9/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\u001b[0m\n",
      "\u001b[36mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[36m#015Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]#015Fetching 13 files:   8%|▊         | 1/13 [00:00<00:01,  7.02it/s]#015Fetching 13 files:  54%|█████▍    | 7/13 [00:02<00:02,  2.96it/s]#015Fetching 13 files: 100%|██████████| 13/13 [00:02<00:00,  5.64it/s]\u001b[0m\n",
      "\u001b[36mWARNING - Overwriting /.sagemaker/mms/models/openai__whisper-base ...\u001b[0m\n",
      "\u001b[36mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[36mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:14,912 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[36mMMS Home: /opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[34mInitial Models: ALL\u001b[0m\n",
      "\u001b[34mLog dir: null\u001b[0m\n",
      "\u001b[34mMetrics dir: null\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 1\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[34mPreload model: false\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:14,984 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,064 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,067 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[36mCurrent directory: /\u001b[0m\n",
      "\u001b[36mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[36mNumber of GPUs: 1\u001b[0m\n",
      "\u001b[36mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[36mMax heap size: 3934 M\u001b[0m\n",
      "\u001b[36mPython executable: /opt/conda/bin/python3.9\u001b[0m\n",
      "\u001b[36mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[36mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[36mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[36mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[36mInitial Models: ALL\u001b[0m\n",
      "\u001b[36mLog dir: null\u001b[0m\n",
      "\u001b[36mMetrics dir: null\u001b[0m\n",
      "\u001b[36mNetty threads: 0\u001b[0m\n",
      "\u001b[36mNetty client threads: 0\u001b[0m\n",
      "\u001b[36mDefault workers per model: 1\u001b[0m\n",
      "\u001b[36mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[36mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[36mMaximum Request Size: 52428800\u001b[0m\n",
      "\u001b[36mPreload model: false\u001b[0m\n",
      "\u001b[36mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:14,984 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-openai__whisper-base\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,064 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/openai__whisper-base --model-name openai__whisper-base --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,067 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 59\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,068 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.9.13\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,069 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,074 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,088 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,161 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,169 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,190 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,474 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:47780 \"GET /ping HTTP/1.1\" 200 13\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:15,504 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47794 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,069 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model openai__whisper-base loaded.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,074 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,088 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,161 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[36mModel server started.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,169 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,190 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,474 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:47780 \"GET /ping HTTP/1.1\" 200 13\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:15,504 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47794 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,020 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:39048 \"GET /ping HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,039 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:39058 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:16,020 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:39048 \"GET /ping HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:16,039 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:39058 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,173 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:16,383 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:16,383 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:17,472 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:17,472 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No inference script implementation was found at `inference`. Default implementation of all functions will be used.\u001b[0m\n",
      "\u001b[35m2025-10-13T23:51:16.049:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:14.803:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=50, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,002 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000021-00000001-a8eebc9d8dfc3b99-7b61a5f6\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,004 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5311\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,005 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:20,222 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000023-00000001-703e2bc48dfc419b-bc5041a9\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:20,223 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5013\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:20,224 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:20,222 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000023-00000001-703e2bc48dfc419b-bc5041a9\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:20,223 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5013\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:20,224 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,508 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000021-00000001-d5a840a78dfc3d8d-6ceaa5ad\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5320\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:19,511 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:19,508 [INFO ] W-9000-openai__whisper-base-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model openai__whisper-base loaded io_fd=0242a9fffefeff83-00000021-00000001-d5a840a78dfc3d8d-6ceaa5ad\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:19,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 5320\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:19,511 [WARN ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-openai__whisper-base-1\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,140 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,141 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,239 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08058547973632812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1726\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 4514\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,240 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1724.715232849121 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,241 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08225440979003906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,140 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,141 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,239 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08058547973632812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1726\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,240 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 4514\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,240 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1724.715232849121 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,241 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08225440979003906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,604 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,605 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,685 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07843971252441406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,686 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1677.2093772888184 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,686 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1679\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,686 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07748603820800781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,687 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 5651\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,958 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:20,959 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,058 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12803077697753906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 295\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 298\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 293.4834957122803 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,059 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,253 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,253 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04458427429199219 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,307 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 233\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 232.2559356689453 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,308 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,308 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06198883056640625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,509 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,811 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,811 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,812 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,910 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07772445678710938 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1682.9359531402588 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,911 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1684\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07867813110351562 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,912 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 5747\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,812 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,910 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07772445678710938 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 1682.9359531402588 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,911 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1684\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,911 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07867813110351562 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:21,912 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 5747\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,203 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,586 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06985664367675781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,586 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 282\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,587 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.0986042022705 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.16546249389648438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 328.7193775177002 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,949 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 330\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,950 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 334\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,285 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5092620849609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,503 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,586 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06985664367675781 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,586 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 282\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,587 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.0986042022705 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,821 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.16546249389648438 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 328.7193775177002 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,949 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 330\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:21,950 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 334\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,169 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,285 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5092620849609375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 311.5224838256836 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,286 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 311.5224838256836 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,286 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,286 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,595 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.36215782165527344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,595 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 267\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,596 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 271\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,596 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 265.71154594421387 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,596 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06198883056640625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,793 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,870 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.23627281188964844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,870 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.0976276397705 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,870 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,871 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06198883056640625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:21,871 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 259\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,078 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,078 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,188 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.17690658569335938 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,188 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 293.0786609649658 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,189 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 294\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,189 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 298\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,386 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,386 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,456 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12183189392089844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,456 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.22857093811035 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,456 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,457 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,457 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 256\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,295 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11277198791503906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 314.8057460784912 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 316\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3116130828857422 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.2490348815918 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,856 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.24056434631347656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 250\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 248.3346462249756 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10323524475097656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.1739845275879 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 265\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,295 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11277198791503906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 314.8057460784912 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 316\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,296 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 319\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,492 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,587 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3116130828857422 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.2490348815918 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,588 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,785 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,856 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.24056434631347656 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 250\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 248.3346462249756 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:22,857 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,060 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10323524475097656 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.1739845275879 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 265\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 268\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,143 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 268\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,490 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,491 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1220703125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 290\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 288.1276607513428 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,595 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 293\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,922 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12111663818359375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 312.7858638763428 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,922 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,923 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 317\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,490 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,491 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1220703125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 290\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 288.1276607513428 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,595 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 293\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,789 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,922 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12111663818359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 312.7858638763428 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,922 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 314\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,923 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:22,923 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 317\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,158 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.14162063598632812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 349\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 347.83411026000977 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 352\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.14162063598632812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 349\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 347.83411026000977 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 352\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,288 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06818771362304688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,669 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,670 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,824 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11396408081054688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,824 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 353\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,824 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 352.68449783325195 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,825 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 357\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:22,825 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,033 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,034 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,172 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8375644683837891 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,172 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 326\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,172 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 323.9307403564453 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,173 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 331\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,173 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,430 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,430 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,341 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,342 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4444122314453125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 290.75098037719727 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,458 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 292\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,459 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,459 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 296\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,840 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.40459632873535156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 368\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 365.7352924346924 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 370\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,040 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,041 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,186 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,186 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 326\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 325.84142684936523 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,187 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 331\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,341 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,342 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4444122314453125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,458 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 290.75098037719727 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,458 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 292\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,459 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,459 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 296\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,698 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,840 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.40459632873535156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 368\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 365.7352924346924 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 370\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:23,841 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,040 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,041 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,186 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,186 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 326\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 325.84142684936523 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,187 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,187 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 331\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,482 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,483 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.091552734375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.9881420135498 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,482 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,483 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.091552734375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.9881420135498 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12040138244628906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.7273750305176 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 271\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,090 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,091 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4343986511230469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 458.73451232910156 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,342 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 460\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,344 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 472\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,363 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,364 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,561 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,763 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12040138244628906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.7273750305176 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:23,845 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 271\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,090 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,091 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4343986511230469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 458.73451232910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,342 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 460\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,344 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 472\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,363 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,364 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.24247169494628906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 365\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 364.0313148498535 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,558 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 369\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,575 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,576 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,756 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,756 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5209445953369141 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 303.7891387939453 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,879 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 305\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,880 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 310\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,899 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:23,899 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.45561790466308594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 304\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 302.69503593444824 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,203 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 311\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,220 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,220 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,402 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,402 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5242824554443359 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 287\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 286.0274314880371 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,506 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 291\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,507 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,521 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07486343383789062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.7702579498291 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06771087646484375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 266\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.9803161621094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 270\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.29850006103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,074 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 320\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 319.06723976135254 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 325\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,376 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07486343383789062 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.7702579498291 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,452 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06771087646484375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,453 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,653 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.25844573974609375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 266\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.9803161621094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,738 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 270\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,755 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:24,936 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.29850006103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,074 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 320\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,074 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 319.06723976135254 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 325\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,095 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,559 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,560 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,692 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2162456512451172 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,692 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 329\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,692 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 328.3960819244385 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,693 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 336\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,693 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,725 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,725 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,982 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,982 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,235 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3688335418701172 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,235 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 511\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,235 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 510.0405216217041 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,235 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06651878356933594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,235 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 520\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,254 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,255 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,559 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,560 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,692 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2162456512451172 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,692 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 329\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,692 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 328.3960819244385 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,693 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 336\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,693 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,725 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,725 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,982 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:24,982 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,235 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3688335418701172 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,235 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 511\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,235 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 510.0405216217041 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,235 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06651878356933594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,235 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 520\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,254 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,255 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,703 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,703 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,780 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3192424774169922 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.8334083557129 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,781 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 264\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,793 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,793 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,983 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:24,983 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,098 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3936290740966797 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,098 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 306\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,098 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 304.4602870941162 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,099 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.18286705017089844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,099 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 309\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,113 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,113 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,293 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,293 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,382 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.29730796813964844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 271\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 269.6421146392822 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 274\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.23174285888671875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 301.8605709075928 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 304\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 308\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8287429809570312 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 306\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 303.7574291229248 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 313\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,740 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,741 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,915 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,916 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2791881561279297 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 246.70004844665527 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 248\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,988 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,170 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,171 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,216 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.20647048950195312 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 220\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 218.7364101409912 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 221\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,251 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,252 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,277 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.23174285888671875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 301.8605709075928 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 304\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,397 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 308\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,419 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,597 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8287429809570312 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 306\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 303.7574291229248 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 313\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,740 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,741 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,915 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,916 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2791881561279297 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 246.70004844665527 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 248\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,987 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,988 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:25,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,170 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,171 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,216 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.20647048950195312 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 220\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 218.7364101409912 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,217 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 221\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,251 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,252 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.7693767547607422 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.3625240325928 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,753 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,450 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.7693767547607422 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.3625240325928 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,546 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,565 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,753 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,754 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5369186401367188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.7336139678955 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,817 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,069 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07605552673339844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,069 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 243\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,070 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.26784706115723 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,086 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,754 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5369186401367188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.7336139678955 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,816 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,817 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:25,827 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,008 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,069 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07605552673339844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,069 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 243\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,070 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.26784706115723 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,070 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,086 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,288 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,607 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,607 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,767 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.27942657470703125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,767 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 358\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,767 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 356.67967796325684 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,768 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06985664367675781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,768 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 367\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,784 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,784 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,970 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:25,970 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.5540847778320312 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 277.0516872406006 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,061 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,062 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 283\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,081 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,082 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,269 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,270 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.7097721099853516 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 259\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 257.4782371520996 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,340 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,350 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,351 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,535 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,535 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.45800209045410156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 341.40968322753906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 343\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 354\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1659393310546875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.27847290039062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12087821960449219 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 245.70322036743164 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05888938903808594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 249\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,446 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.45800209045410156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 341.40968322753906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 343\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,593 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06270408630371094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 354\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,792 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1659393310546875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.27847290039062 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.061511993408203125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,859 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:26,872 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,042 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.12087821960449219 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 245.70322036743164 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05888938903808594 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,118 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 249\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,131 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1838207244873047 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 298\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 296.3528633117676 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,384 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1838207244873047 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 298\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,383 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 296.3528633117676 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,384 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,413 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,615 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,616 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8833408355712891 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 335\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 332.95488357543945 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 343\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,747 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,758 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,943 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,944 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2512931823730469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 258\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.07545471191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,015 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1666545867919922 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 283\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,308 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.6140651702881 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,309 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,320 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,615 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,616 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.8833408355712891 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 335\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 332.95488357543945 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,746 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 343\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,747 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06866455078125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,758 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,943 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:26,944 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2512931823730469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 258\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,014 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.07545471191406 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,015 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,209 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,307 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1666545867919922 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 283\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 281.6140651702881 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 285\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,309 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,320 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,321 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,321 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,584 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04792213439941406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,584 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 233.66951942443848 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,584 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,585 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.07390975952148438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,585 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 237\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,601 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,601 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,786 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,786 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.27060508728027344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 283.9467525482178 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 285\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,885 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,886 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 289\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,896 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:26,897 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,072 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,073 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.0743865966796875 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 233.02865028381348 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06079673767089844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,130 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,145 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,145 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,330 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,331 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,432 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.21982192993164062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,432 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 288\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,432 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 286.83924674987793 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,433 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06461143493652344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,433 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 292\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,440 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,440 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,307 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,308 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,307 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,308 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1575946807861328 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.39399337768555 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,417 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,418 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,602 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,603 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2923011779785156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 276.55887603759766 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06389617919921875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,695 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 283\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,706 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,707 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,978 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09202957153320312 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 272.1548080444336 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 273\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 275\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,228 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.021457672119140625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.38245391845703 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 245\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1575946807861328 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 268.39399337768555 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 270\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,400 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,417 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,418 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,602 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,603 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2923011779785156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 276.55887603759766 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,694 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06389617919921875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,695 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 283\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,706 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,707 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,883 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,978 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09202957153320312 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 272.1548080444336 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 273\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,979 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 275\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:27,985 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,149 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,228 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.021457672119140625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.38245391845703 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,229 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 245\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,239 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,530 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,531 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.34499168395996094 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 276\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 273.68783950805664 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,829 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,830 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 292.675256729126 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 294\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,902 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,084 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.023365020751953125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.49729919433594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,530 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,531 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.34499168395996094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 276\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 273.68783950805664 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,594 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,608 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,829 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,830 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 292.675256729126 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 294\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,901 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,902 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:27,909 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,083 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,084 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.023365020751953125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.49729919433594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,154 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,167 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,345 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,610 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,610 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,667 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.03552436828613281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 229\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 227.4479866027832 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 230\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,668 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,678 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,678 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,855 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,855 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,932 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.024318695068359375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 254.28128242492676 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06318092346191406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,933 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,950 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:27,950 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,132 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,133 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1609325408935547 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.91807746887207 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,202 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,213 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,214 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,392 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,392 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07557868957519531 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 273.2107639312744 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 275\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,487 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 277\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,497 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,497 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,409 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2384185791015625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 232.46383666992188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.058650970458984375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,472 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,473 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.025510787963867188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.01561164855957 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,409 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2384185791015625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 232.46383666992188 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.058650970458984375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,472 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 234\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,473 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 236\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,479 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,647 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.025510787963867188 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.01561164855957 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.059604644775390625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 257\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08130073547363281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 256.0882568359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,002 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 259\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.26607513427734375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.29400062561035 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:28,918 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.08130073547363281 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 256.0882568359375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 257\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,001 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,002 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 259\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,010 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,179 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.26607513427734375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 243.29400062561035 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,254 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,267 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07462501525878906 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.764741897583 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06651878356933594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 239\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,583 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,584 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02956390380859375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.7834014892578 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07462501525878906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.764741897583 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06651878356933594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,403 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 239\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,410 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,583 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,584 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02956390380859375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.7834014892578 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,655 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,663 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.015020370483398438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.87924575805664 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,908 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,909 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 248\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,109 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,110 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.22363662719726562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 285.28714179992676 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 286\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06628036499023438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 290\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,834 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.015020370483398438 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.87924575805664 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,908 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,909 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 248\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:28,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,109 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,110 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.22363662719726562 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 285.28714179992676 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 286\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06628036499023438 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,210 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 290\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,223 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,672 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,673 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.3325939178466797 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 237.379789352417 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 239\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 241\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,746 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,746 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,923 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,923 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.35881996154785156 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.74362754821777 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:28,997 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,011 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,011 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,188 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,188 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.31185150146484375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 255.07211685180664 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.064849853515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,266 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 256\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,267 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,283 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,283 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,462 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,462 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,509 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13756752014160156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 244\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.3837184906006 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,693 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,694 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10085105895996094 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 277.1868705749512 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0591278076171875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 281\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,980 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,981 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,051 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02384185791015625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.9655532836914 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,451 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,509 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13756752014160156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 244\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 242.3837184906006 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,510 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 246\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,522 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,693 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,694 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.10085105895996094 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 277.1868705749512 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,799 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0591278076171875 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 278\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,800 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 281\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,810 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,980 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:29,981 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,051 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.02384185791015625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.9655532836914 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06008148193359375 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,052 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,064 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,248 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4849433898925781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 309.950590133667 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 312\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06842613220214844 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,534 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 314\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.48613548278808594 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.49539375305176 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.051975250244140625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 251\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 249.42398071289062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,268 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,269 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,338 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11539459228515625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.93698501586914 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08511543273925781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,408 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4849433898925781 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 309.950590133667 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 312\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,533 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06842613220214844 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,534 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 314\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,547 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,732 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.48613548278808594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 269\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 272\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 267.49539375305176 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,815 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:29,825 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,005 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.051975250244140625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 251\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 249.42398071289062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06437301635742188 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,075 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,087 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,268 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,269 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,338 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.11539459228515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 253\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.93698501586914 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.08511543273925781 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,352 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,570 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13589859008789062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 289\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 287.6467704772949 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.062465667724609375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,571 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 291\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,582 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,582 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,759 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1404285430908203 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 248\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 246.5190887451172 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06222724914550781 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,829 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 250\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,845 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:29,845 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,025 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,026 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.19431114196777344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 295.8817481994629 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,141 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 300\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,156 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,156 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,338 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,338 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.15592575073242188 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 317.3072338104248 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,473 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 318\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,474 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 323\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,484 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,484 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07414817810058594 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.460636138916 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,328 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,574 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2357959747314453 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 236\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.11481285095215 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.07414817810058594 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,327 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.460636138916 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,328 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06365776062011719 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,328 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,339 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,508 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,574 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2357959747314453 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 236\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.11481285095215 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06127357482910156 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,575 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 247.99561500549316 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 249\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,834 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 251\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2033710479736328 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.48221588134766 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,585 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,761 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 247.99561500549316 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 249\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,833 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06031990051269531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,834 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 251\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:30,843 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,012 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2033710479736328 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 235.48221588134766 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 237\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,079 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 238\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,088 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,559 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,560 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,643 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1201629638671875 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.67771339416504 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,917 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.85486602783203 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,918 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,918 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,926 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,927 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,190 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04887580871582031 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.655424118042 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,559 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,560 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,643 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.1201629638671875 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 293\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 291.67771339416504 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,644 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,838 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.09608268737792969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,917 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 260\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,917 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 258.85486602783203 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,918 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 263\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,918 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06532669067382812 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,926 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:30,927 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,108 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,190 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04887580871582031 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 263.655424118042 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 264\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,191 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 267\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,200 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,657 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,722 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04982948303222656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,722 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 239\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,722 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 238.0218505859375 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,723 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06508827209472656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,723 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 241\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,730 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,730 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,905 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,905 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04935264587402344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 244.45176124572754 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 245\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06604194641113281 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,975 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 247\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,998 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:30,999 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,186 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,186 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,296 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.16736984252929688 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 297.98316955566406 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06628036499023438 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 299\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,297 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 304\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,306 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,306 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,480 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,480 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,270 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,271 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,270 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,271 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06937980651855469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 257.8301429748535 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,347 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,346 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 258\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,347 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 261\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,607 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.05269050598144531 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.55432319641113 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 253\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.15091896057128906 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,879 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.2662410736084 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,880 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4470348358154297 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 297\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 295.4273223876953 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 300\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.06937980651855469 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,346 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 257.8301429748535 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,347 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06103515625 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,346 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 258\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,347 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 261\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,357 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,541 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,607 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.05269050598144531 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 250.55432319641113 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.05936622619628906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,608 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 253\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,628 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,809 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,879 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.15091896057128906 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,879 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.2662410736084 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,880 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 255\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,880 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06175041198730469 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:31,893 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,068 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.4470348358154297 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 297\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 295.4273223876953 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.0629425048828125 ms\u001b[0m\n",
      "\u001b[36m2025-10-13T23:51:32,189 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:47808 \"POST /invocations HTTP/1.1\" 200 300\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04982948303222656 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.0991096496582 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 274.2452621459961 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 275\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13327598571777344 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 299\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 297.41835594177246 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,044 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,263 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,264 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2689361572265625 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 322.7851390838623 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,393 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04982948303222656 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 252\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 251.0991096496582 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06556510925292969 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,451 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 254\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,459 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,658 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2079010009765625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 274.2452621459961 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06580352783203125 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,734 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 275\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,735 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 278\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,745 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:31,925 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.13327598571777344 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 299\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 301\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,043 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 297.41835594177246 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,044 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06413459777832031 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,058 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,263 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,264 [WARN ] W-openai__whisper-base-1-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   warnings.warn(\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.2689361572265625 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 322.7851390838623 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 324\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 328\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06723403930664062 ms\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 324\u001b[0m\n",
      "\u001b[34m2025-10-13T23:51:32,381 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:39068 \"POST /invocations HTTP/1.1\" 200 328\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,570 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Preprocess time - 0.04649162292480469 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,570 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Predict time - 264.5101547241211 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,571 [INFO ] W-openai__whisper-base-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Postprocess time - 0.06341934204101562 ms\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,571 [INFO ] W-9000-openai__whisper-base com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 265\u001b[0m\n",
      "\u001b[32m2025-10-13T23:51:31,571 [INFO ] W-9000-openai__whisper-base ACCESS_LOG - /169.254.255.130:59366 \"POST /invocations HTTP/1.1\" 200 268\u001b[0m\n",
      "Output at: s3://asrelder-data/whisper_batch/output/dementiabank\n"
     ]
    }
   ],
   "source": [
    "# Manifest MUST be in JSON!!!!\n",
    "\n",
    "batch.transform(\n",
    "    data=dementia_manifest_s3,\n",
    "    data_type=\"ManifestFile\",           # IMPORTANT\n",
    "    content_type=\"audio/mpeg\",          # type of the  objects\n",
    "    split_type=\"None\"\n",
    ")\n",
    "batch.wait()\n",
    "print(\"Output at:\", output_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4cd79-8b29-48d7-bcc4-db0ca962c91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
