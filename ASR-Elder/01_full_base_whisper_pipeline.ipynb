{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16376b3f-67d9-44ae-aa30-3c5ae4f3dde0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:23.256550Z",
     "iopub.status.busy": "2025-10-05T20:12:23.256276Z",
     "iopub.status.idle": "2025-10-05T20:12:25.765544Z",
     "shell.execute_reply": "2025-10-05T20:12:25.764719Z",
     "shell.execute_reply.started": "2025-10-05T20:12:23.256526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio-ffmpeg in /opt/conda/lib/python3.12/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fdb82a8-8d24-4c77-8f71-982e590d3fdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:25.767326Z",
     "iopub.status.busy": "2025-10-05T20:12:25.766978Z",
     "iopub.status.idle": "2025-10-05T20:12:25.772753Z",
     "shell.execute_reply": "2025-10-05T20:12:25.771622Z",
     "shell.execute_reply.started": "2025-10-05T20:12:25.767287Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import tempfile\n",
    "import shutil\n",
    "import subprocess\n",
    "import traceback\n",
    "import gc\n",
    "from typing import List, Set, Dict, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a922ef-2169-4c13-b2d8-492ed9df81dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:25.774136Z",
     "iopub.status.busy": "2025-10-05T20:12:25.773779Z",
     "iopub.status.idle": "2025-10-05T20:12:43.122620Z",
     "shell.execute_reply": "2025-10-05T20:12:43.121756Z",
     "shell.execute_reply.started": "2025-10-05T20:12:25.774106Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-10-05 20:12:35.609523: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import torch first to avoid registration conflicts\n",
    "import torch\n",
    "import torchvision  # Import this explicitly before transformers\n",
    "\n",
    "# Env setup\n",
    "import os\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", \"/tmp/transformers_cache\")\n",
    "os.environ.setdefault(\"HF_HOME\", \"/tmp/hf_home\")\n",
    "os.environ.setdefault(\"TORCH_HOME\", \"/tmp/torch_home\")\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import imageio_ffmpeg\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e063c6f4-0ec2-4979-b30b-f758217825d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:43.124523Z",
     "iopub.status.busy": "2025-10-05T20:12:43.123679Z",
     "iopub.status.idle": "2025-10-05T20:12:43.129555Z",
     "shell.execute_reply": "2025-10-05T20:12:43.128493Z",
     "shell.execute_reply.started": "2025-10-05T20:12:43.124496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(\"asr_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b980639-691b-4660-aa25-a31452068d25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:43.131161Z",
     "iopub.status.busy": "2025-10-05T20:12:43.130806Z",
     "iopub.status.idle": "2025-10-05T20:12:43.137403Z",
     "shell.execute_reply": "2025-10-05T20:12:43.136484Z",
     "shell.execute_reply.started": "2025-10-05T20:12:43.131075Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================== Config =============================================== \n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the transcription pipeline\"\"\"\n",
    "    # S3 settings\n",
    "    s3_input: str = \"s3://asrelder-data/common_voice/clips/\"\n",
    "    output_local_csv: str = \"./transcripts_from_prefix.csv\"\n",
    "    write_back_to_s3: bool = False\n",
    "    output_s3_uri: Optional[str] = None \n",
    "    #output_s3_uri: str = \"s3://asrelder-data/outputs/transcripts_from_prefix.csv\" uncomment if we want it back on s3\n",
    "    \n",
    "    # Processing settings\n",
    "    max_files: Optional[int] = 50\n",
    "    download_workers: int = 4\n",
    "    append_every_n: int = 20\n",
    "    resume_from_csv: bool = True\n",
    "    \n",
    "    # Model settings\n",
    "    model_id: str = \"openai/whisper-base\"\n",
    "    language: Optional[str] = None  # change to en\n",
    "    task: str = \"transcribe\"\n",
    "    chunk_length_s: int = 30\n",
    "    stride_length_s: tuple = (5, 5)\n",
    "    \n",
    "    # File settings\n",
    "    audio_extensions: List[str] = field(default_factory=lambda: [\".mp3\", \".wav\", \".flac\", \".m4a\", \".ogg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f386735d-5049-48b6-8ff0-094c5f7c8f64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:43.138721Z",
     "iopub.status.busy": "2025-10-05T20:12:43.138236Z",
     "iopub.status.idle": "2025-10-05T20:12:43.160672Z",
     "shell.execute_reply": "2025-10-05T20:12:43.159642Z",
     "shell.execute_reply.started": "2025-10-05T20:12:43.138690Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================== Core Components =============================================== \n",
    "class FFmpegSetup:\n",
    "    \"\"\"Manages FFmpeg availability\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def ensure_available() -> Optional[str]:\n",
    "        \"\"\"Check if ffmpeg is available on PATH\"\"\"\n",
    "        ff = None\n",
    "        try:\n",
    "            ff = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"imageio-ffmpeg error: {e}\")\n",
    "        \n",
    "        if ff and os.path.exists(ff):\n",
    "            ff_dir = os.path.dirname(ff)\n",
    "            os.environ[\"PATH\"] = ff_dir + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "        \n",
    "        resolved = shutil.which(\"ffmpeg\")\n",
    "        if resolved:\n",
    "            try:\n",
    "                out = subprocess.run(\n",
    "                    [resolved, \"-version\"],\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.STDOUT,\n",
    "                    check=True,\n",
    "                    timeout=5\n",
    "                )\n",
    "                logger.info(f\"ffmpeg: {resolved} | {out.stdout.decode('utf-8').splitlines()[0]}\")\n",
    "            except Exception:\n",
    "                logger.info(f\"ffmpeg: {resolved} (version check failed)\")\n",
    "        else:\n",
    "            # Create shim if needed\n",
    "            if ff and os.path.exists(ff):\n",
    "                bin_dir = os.path.expanduser(\"~/.local/bin\")\n",
    "                os.makedirs(bin_dir, exist_ok=True)\n",
    "                shim = os.path.join(bin_dir, \"ffmpeg\")\n",
    "                with open(shim, \"w\") as f:\n",
    "                    f.write(f\"#!/usr/bin/env bash\\n\\\"{ff}\\\" \\\"$@\\\"\\n\")\n",
    "                os.chmod(shim, 0o755)\n",
    "                os.environ[\"PATH\"] = bin_dir + os.pathsep + os.environ.get(\"PATH\", \"\")\n",
    "                resolved = shutil.which(\"ffmpeg\")\n",
    "                if resolved:\n",
    "                    logger.info(f\"ffmpeg shim created: {resolved}\")\n",
    "        \n",
    "        if not resolved:\n",
    "            logger.warning(\"FFmpeg not found; use torchaudio fallback\")\n",
    "        \n",
    "        return resolved\n",
    "\n",
    "class S3Manager:\n",
    "    \"\"\"Handles S3 operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.client = boto3.client(\"s3\")\n",
    "    \n",
    "    def parse_uri(self, uri: str) -> tuple[str, str]:\n",
    "        \"\"\"Parse S3 URI into bucket and key\"\"\"\n",
    "        if not uri.startswith(\"s3://\"):\n",
    "            raise ValueError(f\"Invalid S3 URI: {uri}\")\n",
    "        p = urlparse(uri)\n",
    "        return p.netloc, p.path.lstrip(\"/\")\n",
    "    \n",
    "    def is_audio_file(self, key: str) -> bool:\n",
    "        \"\"\"Check if key is an audio file\"\"\"\n",
    "        return any(key.lower().endswith(ext) for ext in self.config.audio_extensions)\n",
    "    \n",
    "    def list_audio_keys(self, bucket: str, prefix: str) -> List[str]:\n",
    "        \"\"\"List all audio keys under prefix\"\"\"\n",
    "        if self.is_audio_file(prefix):\n",
    "            return [prefix]\n",
    "        \n",
    "        keys = []\n",
    "        paginator = self.client.get_paginator(\"list_objects_v2\")\n",
    "        \n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if not key.endswith(\"/\") and self.is_audio_file(key):\n",
    "                    keys.append(key)\n",
    "                    if self.config.max_files and len(keys) >= self.config.max_files:\n",
    "                        return keys\n",
    "        return keys\n",
    "    \n",
    "    def download_to_temp(self, bucket: str, key: str) -> str:\n",
    "        \"\"\"Download S3 object to temporary file\"\"\"\n",
    "        _, ext = os.path.splitext(key)\n",
    "        if not ext:\n",
    "            ext = \".mp3\"\n",
    "        \n",
    "        fd, tmp_path = tempfile.mkstemp(suffix=ext)\n",
    "        os.close(fd)\n",
    "        \n",
    "        with open(tmp_path, \"wb\") as f:\n",
    "            self.client.download_fileobj(bucket, key, f)\n",
    "        \n",
    "        return tmp_path\n",
    "    \n",
    "    def upload_file(self, local_path: str, s3_uri: str):\n",
    "        \"\"\"Upload file to S3\"\"\"\n",
    "        bucket, key = self.parse_uri(s3_uri)\n",
    "        self.client.upload_file(local_path, bucket, key)\n",
    "        logger.info(f\"Uploaded to {s3_uri}\")\n",
    "\n",
    "class TranscriptionManager:\n",
    "    \"\"\"Manages the ASR pipeline and transcription\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.pipeline = self._build_pipeline()\n",
    "    \n",
    "    def _build_pipeline(self):\n",
    "        \"\"\"Build the ASR pipeline\"\"\"\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = 0 if use_cuda else -1\n",
    "        dtype = torch.float16 if use_cuda else torch.float32\n",
    "        \n",
    "        generate_kwargs = {}\n",
    "        if self.config.language:\n",
    "            generate_kwargs[\"language\"] = self.config.language\n",
    "        if self.config.task:\n",
    "            generate_kwargs[\"task\"] = self.config.task\n",
    "        \n",
    "        logger.info(f\"Loading ASR: {self.config.model_id} (device={'cuda' if use_cuda else 'cpu'}, dtype={dtype})\")\n",
    "        \n",
    "        return pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.config.model_id,\n",
    "            device=device,\n",
    "            torch_dtype=dtype,\n",
    "            return_timestamps=True,\n",
    "            chunk_length_s=self.config.chunk_length_s,\n",
    "            stride_length_s=self.config.stride_length_s,\n",
    "            generate_kwargs=generate_kwargs or None,\n",
    "        )\n",
    "    \n",
    "    def transcribe(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Transcribe audio file\"\"\"\n",
    "        try:\n",
    "            result = self.pipeline(audio_path)\n",
    "            text = result.get(\"text\", \"\") if isinstance(result, dict) else str(result)\n",
    "            return {\"text\": text, \"error\": None}\n",
    "        except Exception as e:\n",
    "            if \"ffmpeg\" in str(e).lower():\n",
    "                return self._fallback_transcribe(audio_path)\n",
    "            return {\"text\": \"\", \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "    \n",
    "    def _fallback_transcribe(self, audio_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback using torchaudio\"\"\"\n",
    "        try:\n",
    "            import torchaudio\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if waveform.ndim == 2:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "            \n",
    "            result = self.pipeline(waveform.squeeze(0).numpy(), sampling_rate=sr)\n",
    "            text = result.get(\"text\", \"\") if isinstance(result, dict) else str(result)\n",
    "            return {\"text\": text, \"error\": None}\n",
    "        except Exception as e:\n",
    "            return {\"text\": \"\", \"error\": f\"Fallback failed: {e}\"}\n",
    "    \n",
    "    def cleanup_gpu_memory(self):\n",
    "        \"\"\"Clean up GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "class CSVManager:\n",
    "    \"\"\"Handles CSV operations \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "    \n",
    "    def read_processed_keys(self) -> Set[str]:\n",
    "        \"\"\"Read already processed S3 keys from CSV\"\"\"\n",
    "        if not os.path.exists(self.csv_path):\n",
    "            return set()\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path, usecols=[\"s3_key\"])\n",
    "            return set(df[\"s3_key\"].astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read existing CSV: {e}\")\n",
    "            return set()\n",
    "    \n",
    "    def append_results(self, results: List[Dict[str, Any]]):\n",
    "        \"\"\"Append results to CSV\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        mode = \"a\" if os.path.exists(self.csv_path) else \"w\"\n",
    "        header = not os.path.exists(self.csv_path)\n",
    "        \n",
    "        df.to_csv(self.csv_path, index=False, mode=mode, header=header)\n",
    "        logger.info(f\"Appended {len(results)} rows to {self.csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07aced4-ff5e-4ed3-ab3c-7a608599e3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:43.161937Z",
     "iopub.status.busy": "2025-10-05T20:12:43.161608Z",
     "iopub.status.idle": "2025-10-05T20:12:43.175242Z",
     "shell.execute_reply": "2025-10-05T20:12:43.174308Z",
     "shell.execute_reply.started": "2025-10-05T20:12:43.161905Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================================= Main Pipeline ===========================================\n",
    "\n",
    "class ProductionPipeline:\n",
    "    \"\"\"Main for the transcription pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.s3_manager = S3Manager(config)\n",
    "        self.transcription_manager = TranscriptionManager(config)\n",
    "        self.csv_manager = CSVManager(config.output_local_csv)\n",
    "        self.results_buffer: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute the transcription pipeline\"\"\"\n",
    "        # Setup\n",
    "        FFmpegSetup.ensure_available()\n",
    "        \n",
    "        # Get files to process\n",
    "        bucket, prefix = self.s3_manager.parse_uri(self.config.s3_input)\n",
    "        all_keys = self.s3_manager.list_audio_keys(bucket, prefix)\n",
    "        \n",
    "        processed_keys = set()\n",
    "        if self.config.resume_from_csv:\n",
    "            processed_keys = self.csv_manager.read_processed_keys()\n",
    "        \n",
    "        keys_to_process = [k for k in all_keys if k not in processed_keys]\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Processing {len(keys_to_process)} files \"\n",
    "            f\"(skipped {len(processed_keys)} already done)\"\n",
    "        )\n",
    "        \n",
    "        if not keys_to_process:\n",
    "            logger.info(\"No files to process\")\n",
    "            return\n",
    "        \n",
    "        # Process with concurrent downloads\n",
    "        self._process_with_concurrency(bucket, keys_to_process)\n",
    "        \n",
    "        # Final flush\n",
    "        self._flush_results()\n",
    "        \n",
    "        # Upload to S3 if configured\n",
    "        if self.config.write_back_to_s3:\n",
    "            self.s3_manager.upload_file(\n",
    "                self.config.output_local_csv,\n",
    "                self.config.output_s3_uri\n",
    "            )\n",
    "        \n",
    "        logger.info(\"Pipeline complete!\")\n",
    "    \n",
    "    def _process_with_concurrency(self, bucket: str, keys: List[str]):\n",
    "        \"\"\"Process files with concurrent downloads\"\"\"\n",
    "        batch_size = max(1, self.config.download_workers * 2)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.config.download_workers) as pool:\n",
    "            for i in range(0, len(keys), batch_size):\n",
    "                batch = keys[i:i + batch_size]\n",
    "                futures = {\n",
    "                    pool.submit(self.s3_manager.download_to_temp, bucket, k): k\n",
    "                    for k in batch\n",
    "                }\n",
    "                \n",
    "                progress = tqdm(\n",
    "                    as_completed(futures),\n",
    "                    total=len(futures),\n",
    "                    desc=f\"Batch {i//batch_size + 1}\"\n",
    "                )\n",
    "                \n",
    "                for future in progress:\n",
    "                    key = futures[future]\n",
    "                    self._process_single_file(future, key)\n",
    "    \n",
    "    def _process_single_file(self, future, key: str):\n",
    "        \"\"\"Process a single downloaded file\"\"\"\n",
    "        local_path = None\n",
    "        try:\n",
    "            # Get downloaded file\n",
    "            local_path = future.result()\n",
    "            \n",
    "            # Transcribe\n",
    "            result = self.transcription_manager.transcribe(local_path)\n",
    "            \n",
    "            # Store result\n",
    "            self.results_buffer.append({\n",
    "                \"s3_key\": key,\n",
    "                \"filename\": os.path.basename(key),\n",
    "                \"transcribed_text\": result[\"text\"],\n",
    "                \"error\": result[\"error\"] or \"\"\n",
    "            })\n",
    "            \n",
    "            if result[\"text\"]:\n",
    "                preview = result[\"text\"][:100] + \"...\" if len(result[\"text\"]) > 100 else result[\"text\"]\n",
    "                logger.info(f\"✓ {os.path.basename(key)}: {preview}\")\n",
    "            else:\n",
    "                logger.warning(f\"✗ {os.path.basename(key)}: {result['error']}\")\n",
    "            \n",
    "            # Periodic flush\n",
    "            if len(self.results_buffer) >= self.config.append_every_n:\n",
    "                self._flush_results()\n",
    "            \n",
    "            # Memory cleanup\n",
    "            self.transcription_manager.cleanup_gpu_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc(limit=2)\n",
    "            self.results_buffer.append({\n",
    "                \"s3_key\": key,\n",
    "                \"filename\": os.path.basename(key),\n",
    "                \"transcribed_text\": \"\",\n",
    "                \"error\": f\"{type(e).__name__}: {e} | {tb}\"\n",
    "            })\n",
    "            logger.error(f\"Failed to process {key}: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Clean up temp file\n",
    "            if local_path and os.path.exists(local_path):\n",
    "                try:\n",
    "                    os.remove(local_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "    def _flush_results(self):\n",
    "        \"\"\"Flush results buffer to CSV\"\"\"\n",
    "        if self.results_buffer:\n",
    "            self.csv_manager.append_results(self.results_buffer)\n",
    "            self.results_buffer.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f2806d-ee11-47ec-944b-970909977591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T20:12:43.176833Z",
     "iopub.status.busy": "2025-10-05T20:12:43.176453Z",
     "iopub.status.idle": "2025-10-05T20:18:12.365939Z",
     "shell.execute_reply": "2025-10-05T20:18:12.365186Z",
     "shell.execute_reply.started": "2025-10-05T20:12:43.176802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:12:43,667 INFO - Loading ASR: openai/whisper-base (device=cpu, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:12:45,933 INFO - ffmpeg shim created: /home/sagemaker-user/.local/bin/ffmpeg\n",
      "2025-10-05 20:12:46,175 INFO - Processing 50 files (skipped 0 already done)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:12:51,726 INFO - ✓ common_voice_en_100.mp3:  I admit that I'm an alcoholic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  12%|█▎        | 1/8 [00:05<00:41,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:12:56,574 INFO - ✓ common_voice_en_1.mp3:  I'm interested only in the present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  25%|██▌       | 2/8 [00:10<00:31,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:01,940 INFO - ✓ common_voice_en_1000.mp3:  I was disappointed at this inanimate bulk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  38%|███▊      | 3/8 [00:16<00:28,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:07,588 INFO - ✓ common_voice_en_10.mp3:  The boy looked out at the horizon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  50%|█████     | 4/8 [00:21<00:21,  5.35s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:13,220 INFO - ✓ common_voice_en_10000.mp3:  Then, you will look out for correct grammar and pronunciation and speaking to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  62%|██████▎   | 5/8 [00:27<00:16,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:17,904 INFO - ✓ common_voice_en_100000.mp3:  I'm getting trucker ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  75%|███████▌  | 6/8 [00:32<00:10,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:23,200 INFO - ✓ common_voice_en_100002.mp3:  Team 4 will meet up at point B with Team 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1:  88%|████████▊ | 7/8 [00:37<00:05,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:28,143 INFO - ✓ common_voice_en_100001.mp3:  All I want to do is be loud to long.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 8/8 [00:42<00:00,  5.28s/it]\n",
      "Batch 2:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:32,756 INFO - ✓ common_voice_en_100003.mp3:  Yorkshire is my county.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2:  12%|█▎        | 1/8 [00:04<00:32,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:37,617 INFO - ✓ common_voice_en_100004.mp3:  Be sure you spell the name right.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2:  25%|██▌       | 2/8 [00:09<00:28,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:42,421 INFO - ✓ common_voice_en_100006.mp3:  How long since we've seen each other?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2:  38%|███▊      | 3/8 [00:14<00:23,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:47,680 INFO - ✓ common_voice_en_100008.mp3:  This building has an elevator which is necessary for wheelchairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2:  50%|█████     | 4/8 [00:19<00:19,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:52,137 INFO - ✓ common_voice_en_100005.mp3:  The population was increasing exponentially.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2:  62%|██████▎   | 5/8 [00:23<00:14,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:13:56,656 INFO - ✓ common_voice_en_100007.mp3:  I wish they'd stop that practicing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2:  75%|███████▌  | 6/8 [00:28<00:09,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:03,613 INFO - ✓ common_voice_en_100009.mp3:  He felt unsafe like during his first lesson in driving school.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2:  88%|████████▊ | 7/8 [00:35<00:05,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:09,088 INFO - ✓ common_voice_en_10001.mp3:  I found you the argument of yours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 2: 100%|██████████| 8/8 [00:40<00:00,  5.12s/it]\n",
      "Batch 3:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:13,708 INFO - ✓ common_voice_en_100010.mp3:  The descent was disturbingly steep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3:  12%|█▎        | 1/8 [00:04<00:32,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:18,415 INFO - ✓ common_voice_en_100013.mp3:  I need some time to think.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3:  25%|██▌       | 2/8 [00:09<00:27,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:23,296 INFO - ✓ common_voice_en_100012.mp3:  Well, I should think it was sudden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3:  38%|███▊      | 3/8 [00:14<00:23,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:27,991 INFO - ✓ common_voice_en_100014.mp3:  I feel so good I could spit.\n",
      "2025-10-05 20:14:28,015 INFO - Appended 20 rows to ./transcripts_from_prefix.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3:  50%|█████     | 4/8 [00:18<00:18,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:32,181 INFO - ✓ common_voice_en_100015.mp3:  That man is terrific.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3:  62%|██████▎   | 5/8 [00:23<00:13,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:36,958 INFO - ✓ common_voice_en_100011.mp3:  The chairlift took them up the mountain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3:  75%|███████▌  | 6/8 [00:27<00:09,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:41,549 INFO - ✓ common_voice_en_100016.mp3:  Pizza is an Italian classic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3:  88%|████████▊ | 7/8 [00:32<00:04,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:45,958 INFO - ✓ common_voice_en_100017.mp3:  How much do you need?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 3: 100%|██████████| 8/8 [00:36<00:00,  4.61s/it]\n",
      "Batch 4:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:51,535 INFO - ✓ common_voice_en_100020.mp3:  A current of love rushed from his heart and the boy began to pray.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4:  12%|█▎        | 1/8 [00:05<00:38,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:14:56,917 INFO - ✓ common_voice_en_100018.mp3:  The Englishman vanished to gone to find the alchemist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4:  25%|██▌       | 2/8 [00:10<00:32,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:04,737 INFO - ✓ common_voice_en_10002.mp3:  That's the full story of being right to them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4:  38%|███▊      | 3/8 [00:19<00:33,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:10,770 INFO - ✓ common_voice_en_100021.mp3:  Most of them were staring quietly at the big table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4:  50%|█████     | 4/8 [00:24<00:25,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:15,295 INFO - ✓ common_voice_en_100022.mp3:  And then he perceived it very slowly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4:  62%|██████▎   | 5/8 [00:29<00:16,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:21,061 INFO - ✓ common_voice_en_100023.mp3:  The two men hurried back and found the cylinder still lying in the same position.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4:  75%|███████▌  | 6/8 [00:35<00:11,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:26,487 INFO - ✓ common_voice_en_100019.mp3:  Finally, after hours of waiting, the guard bade the boy enter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4:  88%|████████▊ | 7/8 [00:40<00:05,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:32,184 INFO - ✓ common_voice_en_100024.mp3:  At other times, at a crucial moment, I make it easier for things to happen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 4: 100%|██████████| 8/8 [00:46<00:00,  5.78s/it]\n",
      "Batch 5:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:37,653 INFO - ✓ common_voice_en_100026.mp3:  They lock themselves in their laboratories and try to evolve as gold had.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  12%|█▎        | 1/8 [00:05<00:38,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:43,779 INFO - ✓ common_voice_en_100027.mp3:  The land was ruined and I had to find some other way to earn a living.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  25%|██▌       | 2/8 [00:11<00:35,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:48,181 INFO - ✓ common_voice_en_100028.mp3:  Thank you very much.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  38%|███▊      | 3/8 [00:15<00:25,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:53,587 INFO - ✓ common_voice_en_100025.mp3:  The alchemist told the boy to place the shell over his ear.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  50%|█████     | 4/8 [00:21<00:21,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:15:59,476 INFO - ✓ common_voice_en_10003.mp3:  They're not really things, they just work together and talk time to time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  62%|██████▎   | 5/8 [00:27<00:16,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:16:06,574 INFO - ✓ common_voice_en_100029.mp3:  The desert was all sand in some stretches, and rocked in rivers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  75%|███████▌  | 6/8 [00:34<00:12,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:16:12,100 INFO - ✓ common_voice_en_100031.mp3:  Thank you for watching and I'll see you in the next video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5:  88%|████████▊ | 7/8 [00:39<00:05,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:16:18,278 INFO - ✓ common_voice_en_100030.mp3:  I've been looking for you all morning, he said, as he led the boy outside.\n",
      "2025-10-05 20:16:18,281 INFO - Appended 20 rows to ./transcripts_from_prefix.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 5: 100%|██████████| 8/8 [00:46<00:00,  5.76s/it]\n",
      "Batch 6:   0%|          | 0/8 [00:00<?, ?it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:24,931 INFO - ✓ common_voice_en_100033.mp3:  I need all of that, I need all of that, I need all of that, I need all of that, I need all of that,...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6:  12%|█▎        | 1/8 [01:06<07:46, 66.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:29,757 INFO - ✓ common_voice_en_100034.mp3:  I just know that the tradition is always right.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6:  25%|██▌       | 2/8 [01:11<03:01, 30.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:35,740 INFO - ✓ common_voice_en_100035.mp3:  I saw a young man standing on a cylinder and trying to scramble out of the hole again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6:  38%|███▊      | 3/8 [01:17<01:35, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:40,312 INFO - ✓ common_voice_en_100032.mp3:  Thank you very much.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6:  50%|█████     | 4/8 [01:22<00:53, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:44,919 INFO - ✓ common_voice_en_100036.mp3:  I forgot my hat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6:  62%|██████▎   | 5/8 [01:26<00:30, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:49,478 INFO - ✓ common_voice_en_100037.mp3:  Muhammad plays often with the computer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6:  75%|███████▌  | 6/8 [01:31<00:16,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:53,798 INFO - ✓ common_voice_en_100039.mp3:  And my name is Michael.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6:  88%|████████▊ | 7/8 [01:35<00:07,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:17:59,933 INFO - ✓ common_voice_en_100038.mp3:  Why does Milisandre look like she wants to consume Jon Snow on the right at the wall?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 6: 100%|██████████| 8/8 [01:41<00:00, 12.72s/it]\n",
      "Batch 7:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:18:07,305 INFO - ✓ common_voice_en_10004.mp3:  The rule has been long because Paris.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 7:  50%|█████     | 1/2 [00:07<00:07,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:18:12,001 INFO - ✓ common_voice_en_100040.mp3:  The burning fire had been extinguished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 7: 100%|██████████| 2/2 [00:11<00:00,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-05 20:18:12,332 INFO - Appended 10 rows to ./transcripts_from_prefix.csv\n",
      "2025-10-05 20:18:12,333 INFO - Pipeline complete!\n",
      "Pipeline completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================== Run main loop ========================================\n",
    "def main():\n",
    "    try:\n",
    "        config = Config()\n",
    "        pipeline = ProductionPipeline(config)\n",
    "        pipeline.run()\n",
    "        return 0\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Interrupted by user\")\n",
    "        return 130\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    if result == 0:\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(f\"Pipeline failed with code: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
